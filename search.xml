<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(五)：偏差和方差]]></title>
    <url>%2F2018%2F05%2F23%2F180523-%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[在本练习中，您将实现正则化的线性回归和多项式回归，并使用它来研究具有不同偏差-方差属性的模型 1 Regularized Linear Regression 正则线性回归在前半部分的练习中，你将实现正则化线性回归，以预测水库中的水位变化，从而预测大坝流出的水量。在下半部分中，您将通过一些调试学习算法的诊断，并检查偏差 v.s. 方差的影响。 1.1 Visualizing the dataset我们将从可视化数据集开始，其中包含水位变化的历史记录，x，以及从大坝流出的水量，y。 这个数据集分为了三个部分： training set 训练集：训练模型 cross validation set 交叉验证集：选择正则化参数 test set 测试集：评估性能，模型训练中不曾用过的样本 12345%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatimport scipy.optimize as opt 读取数据 123456789101112131415path = 'ex5data1.mat'data = loadmat(path)#Training setX, y = data['X'], data['y']#Cross validation setXval, yval = data['Xval'], data['yval']#Test setXtest, ytest = data['Xtest'], data['ytest']#Insert a column of 1's to all of the X's, as usualX = np.insert(X ,0,1,axis=1)Xval = np.insert(Xval ,0,1,axis=1)Xtest = np.insert(Xtest,0,1,axis=1)print('X=&#123;&#125;,y=&#123;&#125;'.format(X.shape, y.shape))print('Xval=&#123;&#125;,yval=&#123;&#125;'.format(Xval.shape, yval.shape))print('Xtest=&#123;&#125;,ytest=&#123;&#125;'.format(Xtest.shape, ytest.shape)) X=(12, 2),y=(12, 1) Xval=(21, 2),yval=(21, 1) Xtest=(21, 2),ytest=(21, 1) 123456789def plotData(): """瞧一瞧数据长啥样""" plt.figure(figsize=(8,5)) plt.scatter(X[:,1:], y, c='r', marker='x') plt.xlabel('Change in water level (x)') plt.ylabel('Water flowing out of the dam (y)') plt.grid(True) plotData() 1.2 Regularized linear regression cost function 123456789def costReg(theta, X, y, l): '''do not regularizethe theta0 theta is a 1-d array with shape (n+1,) X is a matrix with shape (m, n+1) y is a matrix with shape (m, 1) ''' cost = ((X @ theta - y.flatten()) ** 2).sum() regterm = l * (theta[1:] @ theta[1:]) return (cost + regterm) / (2 * len(X)) Using theta initialized at [1, 1], and lambda = 1, you should expect to see an output of 303.993192 12theta = np.ones(X.shape[1])print(costReg(theta, X, y, 1)) # 303.9931922202643 1.3 Regularized linear regression gradient 12345678910111213141516def gradientReg(theta, X, y, l): """ theta: 1-d array with shape (2,) X: 2-d array with shape (12, 2) y: 2-d array with shape (12, 1) l: lambda constant grad has same shape as theta (2,) """ grad = (X @ theta - y.flatten()) @ X regterm = l * theta regterm[0] = 0 # #don't regulate bias term return (grad + regterm) / len(X)# Using theta initialized at [1; 1] you should expect to see a # gradient of [-15.303016; 598.250744] (with lambda=1)print(gradientReg(theta, X, y, 1)) 1.4 Fitting linear regression 拟合线性回归12345678def trainLinearReg(X, y, l): theta = np.zeros(X.shape[1]) res = opt.minimize(fun=costReg, x0=theta, args=(X, y ,l), method='TNC', jac=gradientReg) return res.x 123fit_theta = trainLinearReg(X, y, 0)plotData()plt.plot(X[:,1], X @ fit_theta) 这里我们把$\lambda$ = 0，因为我们现在实现的线性回归只有两个参数，这么低的维度，正则化并没有用。 从图中可以看到，拟合最好的这条直线告诉我们这个模型并不适合这个数据。 在下一节中，您将实现一个函数来生成学习曲线，它可以帮助您调试学习算法，即使可视化数据不那么容易。 2 Bias-variance机器学习中一个重要的概念是偏差（bias）和方差（variance）的权衡。高偏差意味着欠拟合，高方差意味着过拟合。 在这部分练习中，您将在学习曲线上绘制训练误差和验证误差，以诊断bias-variance问题。 2.1 Learning curves 学习曲线 训练样本X从1开始逐渐增加，训练出不同的参数向量θ。接着通过交叉验证样本Xval计算验证误差。 使用训练集的子集来训练模型，得到不同的theta。 通过theta计算训练代价和交叉验证代价，切记此时不要使用正则化，将 $\lambda = 0$。 计算交叉验证代价时记得整个交叉验证集来计算，无需分为子集。 12345678910111213141516171819def plot_learning_curve(X, y, Xval, yval, l): """画出学习曲线，即交叉验证误差和训练误差随样本数量的变化的变化""" xx = range(1, len(X) + 1) # at least has one example training_cost, cv_cost = [], [] for i in xx: res = trainLinearReg(X[:i], y[:i], l) training_cost_i = costReg(res, X[:i], y[:i], 0) cv_cost_i = costReg(res, Xval, yval, 0) training_cost.append(training_cost_i) cv_cost.append(cv_cost_i) plt.figure(figsize=(8,5)) plt.plot(xx, training_cost, label='training cost') plt.plot(xx, cv_cost, label='cv cost') plt.legend() plt.xlabel('Number of training examples') plt.ylabel('Error') plt.title('Learning curve for linear regression') plt.grid(True) 1learningCurve(X, y, Xval, yval, 0) 从图中看出来，随着样本数量的增加，训练误差和交叉验证误差都很高，这属于高偏差，欠拟合。 3 Polynomial regression 多项式回归我们的线性模型对于数据来说太简单了，导致了欠拟合(高偏差)。在这一部分的练习中，您将通过添加更多的特性来解决这个问题。 使用多项式回归，假设函数形式如下： 3.1 Learning Polynomial Regression数据预处理 X，Xval，Xtest都需要添加多项式特征，这里我们选择增加到6次方，因为若选8次方无法达到作业pdf上的效果图，这是因为scipy和octave版本的优化算法不同。 不要忘了标准化。 12345678910111213141516171819202122def genPolyFeatures(X, power): """添加多项式特征 每次在array的最后一列插入第二列的i+2次方（第一列为偏置） 从二次方开始开始插入（因为本身含有一列一次方） """ Xpoly = X.copy() for i in range(2, power + 1): Xpoly = np.insert(Xpoly, Xpoly.shape[1], np.power(Xpoly[:,1], i), axis=1) return Xpolydef get_means_std(X): """获取训练集的均值和误差，用来标准化所有数据。""" means = np.mean(X,axis=0) stds = np.std(X,axis=0,ddof=1) # ddof=1 means 样本标准差 return means, stdsdef featureNormalize(myX, means, stds): """标准化""" X_norm = myX.copy() X_norm[:,1:] = X_norm[:,1:] - means[1:] X_norm[:,1:] = X_norm[:,1:] / stds[1:] return X_norm 关于归一化，所有数据集应该都用训练集的均值和样本标准差处理。切记。所以要将训练集的均值和样本标准差存储起来，对后面的数据进行处理。 而且注意这里是样本标准差而不是总体标准差，使用np.std()时，将ddof=1则是样本标准差，默认=0是总体标准差。而pandas默认计算样本标准差。 获取添加多项式特征以及 标准化之后的数据。 123456power = 6 # 扩展到x的6次方train_means, train_stds = get_means_std(genPolyFeatures(X,power))X_norm = featureNormalize(genPolyFeatures(X,power), train_means, train_stds)Xval_norm = featureNormalize(genPolyFeatures(Xval,power), train_means, train_stds)Xtest_norm = featureNormalize(genPolyFeatures(Xtest,power), train_means, train_stds) 1234567891011def plot_fit(means, stds, l): """画出拟合曲线""" theta = trainLinearReg(X_norm,y, l) x = np.linspace(-75,55,50) xmat = x.reshape(-1, 1) xmat = np.insert(xmat,0,1,axis=1) Xmat = genPolyFeatures(xmat, power) Xmat_norm = featureNormalize(Xmat, means, stds) plotData() plt.plot(x, Xmat_norm@theta,'b--') 12plot_fit(train_means, train_stds, 0)plot_learning_curve(X_norm, y, Xval_norm, yval, 0) 3.2 Adjusting the regularization parameter上图可以看到 $\lambda$ = 0时，训练误差太小了，明显过拟合了。 我们继续调整$\lambda$ = 1 时： 12plot_fit(train_means, train_stds, 1)plot_learning_curve(X_norm, y, Xval_norm, yval, 1) 我们继续调整$\lambda$ = 100 时，很明显惩罚过多，欠拟合了 12plot_fit(train_means, train_stds, 100)plot_learning_curve(X_norm, y, Xval_norm, yval, 100) 3.3 Selecting λ using a cross validation set1234567891011121314lambdas = [0., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1., 3., 10.]errors_train, errors_val = [], []for l in lambdas: theta = trainLinearReg(X_norm, y, l) errors_train.append(costReg(theta,X_norm,y,0)) # 记得把lambda = 0 errors_val.append(costReg(theta,Xval_norm,yval,0)) plt.figure(figsize=(8,5))plt.plot(lambdas,errors_train,label='Train')plt.plot(lambdas,errors_val,label='Cross Validation')plt.legend()plt.xlabel('lambda')plt.ylabel('Error')plt.grid(True) 12# 可以看到时交叉验证代价最小的是 lambda = 3lambdas[np.argmin(errors_val)] # 3.0 3.4 Computing test set errorIn our cross validation, we obtained a test error of 3.8599 for λ = 3. 实际上我在上面调整了power=6来匹配作业里面的图，所以得不到3.8599。但是调整power=8时（同作业里一样）,就可以得到上述数据。 12345theta = trainLinearReg(X_norm, y, 3)print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(3, costReg(theta, Xtest_norm, ytest, 0)))# for l in lambdas:# theta = trainLinearReg(X_norm, y, l)# print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(l, costReg(theta, Xtest_norm, ytest, 0))) test cost(l=3) = 4.7552720391599]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(四)：神经网络反向传播]]></title>
    <url>%2F2018%2F05%2F21%2F180521-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1 Neural Networks 神经网络在这个练习中，你将实现反向传播算法来学习神经网络的参数。依旧是上次预测手写数数字的例子。 1.1 Visualizing the data 可视化数据这部分我们随机选取100个样本并可视化。训练集共有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。 12345import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatimport scipy.optimize as optfrom sklearn.metrics import classification_report # 这个包是评价报告 1234567def load_mat(path): '''读取数据''' data = loadmat('ex4data1.mat') # return a dict X = data['X'] y = data['y'].flatten() return X, y 1234567891011def plot_100_images(X): """随机画100个数字""" index = np.random.choice(range(5000), 100) images = X[index] fig, ax_array = plt.subplots(10, 10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow(images[r*10 + c].reshape(20,20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 12X,y = load_mat('ex4data1.mat')plot_100_images(X) 1.2 Model representation 模型表示我们的网络有三层，输入层，隐藏层，输出层。我们的输入是数字图像的像素值，因为每个数字的图像大小为20*20，所以我们输入层有400个单元（这里不包括总是输出要加一个偏置单元）。 1.2.1 load train data set 读取数据首先我们要将标签值（1，2，3，4，…，10）转化成非线性相关的向量，向量对应位置（y[i-1]）上的值等于1，例如y[0]=6转化为y[0]=[0,0,0,0,0,1,0,0,0,0]。 123456789101112131415from sklearn.preprocessing import OneHotEncoderdef expand_y(y): result = [] # 把y中每个类别转化为一个向量，对应的lable值在向量对应位置上置为1 for i in y: y_array = np.zeros(10) y_array[i-1] = 1 result.append(y_array) ''' # 或者用sklearn中OneHotEncoder函数 encoder = OneHotEncoder(sparse=False) # return a array instead of matrix y_onehot = encoder.fit_transform(y.reshape(-1,1)) return y_onehot ''' return np.array(result) 获取训练数据集，以及对训练集做相应的处理，得到我们的input X，lables y。 1234567raw_X, raw_y = load_mat('ex4data1.mat')X = np.insert(raw_X, 0, 1, axis=1)y = expand_y(raw_y)X.shape, y.shape'''((5000, 401), (5000, 10))''' 1.2.2 load weight 读取权重这里我们提供了已经训练好的参数θ1，θ2，存储在ex4weight.mat文件中。这些参数的维度由神经网络的大小决定，第二层有25个单元，输出层有10个单元(对应10个数字类)。 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123t1, t2 = load_weight('ex4weights.mat')t1.shape, t2.shape# ((25, 401), (10, 26)) 1.2.3 展开参数当我们使用高级优化方法来优化神经网络时，我们需要将多个参数矩阵展开，才能传入优化函数，然后再恢复形状。 123def serialize(a, b): '''展开参数''' return np.r_[a.flatten(),b.flatten()] 12theta = serialize(t1, t2) # 扁平化参数，25*401+10*26=10285theta.shape # (10285,) 123def deserialize(seq): '''提取参数''' return seq[:25*401].reshape(25, 401), seq[25*401:].reshape(10, 26) 1.3 Feedforward and cost function 前馈和代价函数1.3.1 Feedforward确保每层的单元数，注意输出时加一个偏置单元，s(1)=400+1，s(2)=25+1，s(3)=10。 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 1234567891011def feed_forward(theta, X,): '''得到每层的输入和输出''' t1, t2 = deserialize(theta) # 前面已经插入过偏置单元，这里就不用插入了 a1 = X z2 = a1 @ t1.T a2 = np.insert(sigmoid(z2), 0, 1, axis=1) z3 = a2 @ t2.T a3 = sigmoid(z3) return a1, z2, a2, z3, a3 1a1, z2, a2, z3, h = feed_forward(theta, X) 1.3.2 Cost function回顾下神经网络的代价函数（不带正则化项） 输出层输出的是对样本的预测，包含5000个数据，每个数据对应了一个包含10个元素的向量，代表了结果有10类。在公式中，每个元素与log项对应相乘。 最后我们使用提供训练好的参数θ，算出的cost应该为0.287629 1234567891011121314def cost(theta, X, y): a1, z2, a2, z3, h = feed_forward(theta, X) J = 0 for i in range(len(X)): first = - y[i] * np.log(h[i]) second = (1 - y[i]) * np.log(1 - h[i]) J = J + np.sum(first - second) J = J / len(X) return J''' # or just use verctorization J = - y * np.log(h) - (1 - y) * np.log(1 - h) return J.sum() / len(X)''' 1cost(theta, X, y) # 0.2876291651613189 1.4 Regularized cost function 正则化代价函数 注意不要将每层的偏置项正则化。 最后You should see that the cost is about 0.383770 12345def regularized_cost(theta, X, y, l=1): '''正则化时忽略每层的偏置项，也就是参数矩阵的第一列''' t1, t2 = deserialize(theta) reg = np.sum(t1[:,1:] ** 2) + np.sum(t2[:,1:] ** 2) # or use np.power(a, 2) return l / (2 * len(X)) * reg + cost(theta, X, y) 1regularized_cost(theta, X, y, 1) # 0.38376985909092354 2 Backpropagation 反向传播2.1 Sigmoid gradient S函数导数 这里可以手动推导，并不难。 12def sigmoid_gradient(z): return sigmoid(z) * (1 - sigmoid(z)) 2.2 Random initialization 随机初始化当我们训练神经网络时，随机初始化参数是很重要的，可以打破数据的对称性。一个有效的策略是在均匀分布(−e，e)中随机选择值，我们可以选择 e = 0.12 这个范围的值来确保参数足够小，使得训练更有效率。 123def random_init(size): '''从服从的均匀分布的范围中随机返回size大小的值''' return np.random.uniform(-0.12, 0.12, size) 2.3 Backpropagation 反向传播 目标：获取整个网络代价函数的梯度。以便在优化算法中求解。 这里面一定要理解正向传播和反向传播的过程，才能弄清楚各种参数在网络中的维度，切记。比如手写出每次传播的式子。 123456789101112print('a1', a1.shape,'t1', t1.shape)print('z2', z2.shape)print('a2', a2.shape, 't2', t2.shape)print('z3', z3.shape)print('a3', h.shape)'''a1 (5000, 401) t1 (25, 401)z2 (5000, 25)a2 (5000, 26) t2 (10, 26)z3 (5000, 10)a3 (5000, 10)''' 1234567891011121314def gradient(theta, X, y): ''' unregularized gradient, notice no d1 since the input layer has no error return 所有参数theta的梯度，故梯度D(i)和参数theta(i)同shape，重要。 ''' t1, t2 = deserialize(theta) a1, z2, a2, z3, h = feed_forward(theta, X) d3 = h - y # (5000, 10) d2 = d3 @ t2[:,1:] * sigmoid_gradient(z2) # (5000, 25) D2 = d3.T @ a2 # (10, 26) D1 = d2.T @ a1 # (25, 401) D = (1 / len(X)) * serialize(D1, D2) # (10285,) return D 2.4 Gradient checking 梯度检测在你的神经网络,你是最小化代价函数J(Θ)。执行梯度检查你的参数,你可以想象展开参数Θ(1)Θ(2)成一个长向量θ。通过这样做,你能使用以下梯度检查过程。 123456789101112131415161718192021def gradient_checking(theta, X, y, e): def a_numeric_grad(plus, minus): """ 对每个参数theta_i计算数值梯度，即理论梯度。 """ return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * 2) numeric_grad = [] for i in range(len(theta)): plus = theta.copy() # deep copy otherwise you will change the raw theta minus = theta.copy() plus[i] = plus[i] + e minus[i] = minus[i] - e grad_i = a_numeric_grad(plus, minus) numeric_grad.append(grad_i) numeric_grad = np.array(numeric_grad) analytic_grad = regularized_gradient(theta, X, y) diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad) print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'.format(diff)) 1gradient_checking(theta, X, y, epsilon= 0.0001)#这个运行很慢，谨慎运行 2.5 Regularized Neural Networks 正则化神经网络 12345678910def regularized_gradient(theta, X, y, l=1): """不惩罚偏置单元的参数""" a1, z2, a2, z3, h = feed_forward(theta, X) D1, D2 = deserialize(gradient(theta, X, y)) t1[:,0] = 0 t2[:,0] = 0 reg_D1 = D1 + (l / len(X)) * t1 reg_D2 = D2 + (l / len(X)) * t2 return serialize(reg_D1, reg_D2) 2.6 Learning parameters using fmincg 优化参数12345678910def nn_training(X, y): init_theta = random_init(10285) # 25*401 + 10*26 res = opt.minimize(fun=regularized_cost, x0=init_theta, args=(X, y, 1), method='TNC', jac=regularized_gradient, options=&#123;'maxiter': 400&#125;) return res 1234567891011121314res = nn_training(X, y)#慢res''' fun: 0.5156784004838036 jac: array([-2.51032294e-04, -2.11248326e-12, 4.38829369e-13, ..., 9.88299811e-05, -2.59923586e-03, -8.52351187e-04]) message: 'Converged (|f_n-f_(n-1)| ~= 0)' nfev: 271 nit: 17 status: 1 success: True x: array([ 0.58440213, -0.02013683, 0.1118854 , ..., -2.8959637 , 1.85893941, -2.78756836])''' 1234def accuracy(theta, X, y): _, _, _, _, h = feed_forward(res.x, X) y_pred = np.argmax(h, axis=1) + 1 print(classification_report(y, y_pred)) 1234567891011121314151617accuracy(res.x, X, raw_y)''' precision recall f1-score support 1 0.97 0.99 0.98 500 2 0.98 0.97 0.98 500 3 0.98 0.95 0.96 500 4 0.98 0.97 0.97 500 5 0.97 0.98 0.97 500 6 0.99 0.98 0.98 500 7 0.99 0.97 0.98 500 8 0.96 0.98 0.97 500 9 0.97 0.98 0.97 500 10 0.99 0.99 0.99 500avg / total 0.98 0.98 0.98 5000''' 3 Visualizing the hidden layer 可视化隐藏层理解神经网络是如何学习的一个很好的办法是，可视化隐藏层单元所捕获的内容。通俗的说，给定一个的隐藏层单元，可视化它所计算的内容的方法是找到一个输入x，x可以激活这个单元（也就是说有一个激活值 $a^{(l)}_i$ 接近与1）。对于我们所训练的网络，注意到θ1中每一行都是一个401维的向量，代表每个隐藏层单元的参数。如果我们忽略偏置项，我们就能得到400维的向量，这个向量代表每个样本输入到每个隐层单元的像素的权重。因此可视化的一个方法是，reshape这个400维的向量为（20，20）的图像然后输出。 注：It turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a norm constraint on the input. 这相当于找到了一个输入，给了隐层单元最高的激活值，给定了一个输入的标准限制。例如( $||x||_2 \leq 1$) (这部分暂时不太理解) 12345678910def plot_hidden(theta): t1, _ = deserialize(theta) t1 = t1[:, 1:] fig,ax_array = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(6,6)) for r in range(5): for c in range(5): ax_array[r, c].matshow(t1[r * 5 + c].reshape(20, 20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1plot_hidden(res.x) 在我们经过训练的网络中，你应该发现隐藏的单元大致对应于在输入中寻找笔画和其他模式的检测器。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络(反向传播)详细推导]]></title>
    <url>%2F2018%2F05%2F21%2F180521-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[这篇文章主要讨论神经网络的反向传播的细节，“误差”是如何反向传播的，我们又是如何利用梯度来优化参数的。 在学吴恩达机器学习视频的神经网络那节时，给出了许多公式，比如计算每层的误差，每层参数的梯度，但并没有给出推导过程，可能也是考虑入门级，大多人并不要知道其中含义就可以运用算法了。接下来我会给出详细的推导过程，帮助大家理解。 注意接下来所讲是未正则化的神经网络。 1 计算公式1.1 正向传递假设现在有一个三层的神经网络，如图： 参数含义： $\theta^{(i)}$ 第 $i$ 层的参数矩阵 $z^{(l)}$ 第 $l$ 层的输入 $a^{(l)}$ 第 $l$ 层的输出 传递过程： $a^{(1)}=x​$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ 其中$g$ 为sigmoid激活函数。 1.2 反向传播我们用$\delta^{(l)}$ 表示每层的”误差“，$y$ 为每个样本的标签，$h$ 为每个样本的预测值。 先来从后往前计算每层的“误差“。注意到这里的误差用双引号括起来，因为并不是真正的误差。 $\delta^{(3)}=h-y$ (1) $\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}g^{‘}(z^{(2)})$ (2) 注意第一层是没有误差的，因为是输入层。 吴恩达在课里面提到，”误差“的实质是 $\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}$ ，没错，后面详细说明。 然后来计算每层参数矩阵的梯度，用$\Delta^{(l)}$ 表示 $\Delta^{(2)}=a^{(2)}\delta^{(3)}$ (3) $\Delta^{(1)}=a^{(1)}\delta^{(2)}$ (4) 最后网络的总梯度为： $D=\frac{1}{m}(\Delta^{(1)}+\Delta^{(2)})$ (5) 到这里反向传播就完成了，接着就可以利用梯度下降法或者更高级的优化算法来训练网络。 2 推导这里只推导 $\delta\;和\;\Delta$ 是怎么来的，其余的比较好理解。 首先明确我们要优化的参数有 $\theta^{(1)}$，$\theta^{(2)}$ ，利用梯度下降法的思想，我们只需要求解出代价函数对参数的梯度即可。 假设只有一个输入样本，则代价函数是：$$J(\theta)=-ylogh(x)-(1-y)log(1-h)$$回顾下正向传递的过程，理解其中函数的嵌套关系： $a^{(1)}=x​$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ 然后我们来求解代价函数对参数的梯度，$\frac{\partial}{\partial \theta^{(2)}}J(\theta)$ ，$\frac{\partial}{\partial \theta^{(1)}}J(\theta)$ 。 根据链式求导法则，可以计算得到： 把我画红线的地方令为$\delta^{(3)}$ ，是不是就得到了反向传播中的公式（1）？ 把画绿线的部分令为$\Delta^{(2)}$ ，就得到了公式（3）。我们接着算： 同样把红线部分令为$\delta^{(3)}$ ，紫色部分令为$\delta^{(2)}$ ，就得到了公式（2）。 绿线部分令为$\Delta^{(1)}$ ，就得到了公式（4）。 至此，推导完毕。得到这个规律后，便可以应用到深层次的网络中，计算反向传播时就很方便了。 上面的公式因为书写麻烦，便只写了结果。如果你用笔去慢慢推几分钟，会发现其实很简单。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(三)：多类分类和前馈神经网络]]></title>
    <url>%2F2018%2F05%2F18%2F180518-logistic%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[在此练习中，我们将使用logistic回归和神经网络来识别手写数字（0到9）。 1 多类分类(多个logistic回归)我们将扩展我们在练习2中写的logistic回归的实现，并将其应用于一对多的分类（不止两个类别）。 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.io import loadmat Dataset首先，加载数据集。这里的数据为MATLAB的格式，所以要使用SciPy.io的loadmat函数。 12345def load_data(path): data = loadmat(path) X = data['X'] y = data['y'] return X,y 12345X, y = load_data('ex3data1.mat')print(np.unique(y)) # 看下有几类标签# [ 1 2 3 4 5 6 7 8 9 10]X.shape, y.shape# ((5000, 400), (5000, 1)) 其中有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。 第一个任务是将我们的逻辑回归实现修改为完全向量化（即没有“for”循环）。这是因为向量化代码除了简洁外，还能够利用线性代数优化，并且通常比迭代代码快得多。 1.2 Visualizing the data123456789101112def plot_an_image(X): """ 随机打印一个数字 """ pick_one = np.random.randint(0, 5000) image = X[pick_one, :] fig, ax = plt.subplots(figsize=(1, 1)) ax.matshow(image.reshape((20, 20)), cmap='gray_r') plt.xticks([]) # 去除刻度，美观 plt.yticks([]) plt.show() print('this should be &#123;&#125;'.format(y[pick_one])) 12345678910111213141516def plot_100_image(X): """ 随机画100个数字 """ sample_idx = np.random.choice(np.arange(X.shape[0]), 100) # 随机选100个样本 sample_images = X[sample_idx, :] # (100,400) fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for row in range(10): for column in range(10): ax_array[row, column].matshow(sample_images[10 * row + column].reshape((20, 20)), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1.3 Vectorizing Logistic Regression我们将使用多个one-vs-all(一对多)logistic回归模型来构建一个多类分类器。由于有10个类，需要训练10个独立的分类器。为了提高训练效率，重要的是向量化。在本节中，我们将实现一个不使用任何for循环的向量化的logistic回归版本。 首先准备下数据。 1.3.1 Vectorizing the cost function首先写出向量化的代价函数。回想正则化的logistic回归的代价函数是： $$ J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j $$ 首先我们对每个样本 $i$ 要计算$h_{\theta}(x^{(i)})$，$h_{\theta}(x^{(i)})=g(\theta^Tx^{(i)})$，$g(z)=\frac{1}{1+e^{-z}}$ sigmoid函数。 事实上我们可以对所有的样本用矩阵乘法来快速的计算。让我们如下来定义 $X$ 和 $\theta$ ： 然后通过计算矩阵积 $X\theta$ ，我们可以得到： 在最后一个等式中，我们用到了一个定理，如果 $a$ 和 $b$ 都是向量，那么 $a^Tb=b^Ta$，这样我们就可以用一行代码计算出所有的样本。 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 123456789101112def regularized_cost(theta, X, y, l): """ don't penalize theta_0 args: X: feature matrix, (m, n+1) # 插入了x0=1 y: target vector, (m, ) l: lambda constant for regularization """ thetaReg = theta[1:] first = (-y*np.log(sigmoid(X@theta))) + (y-1)*np.log(1-sigmoid(X@theta)) reg = (thetaReg@thetaReg)*l / (2*len(X)) return np.mean(first) + reg 1.3.2 Vectorizing the gradient回顾正则化logistic回归代价函数的梯度下降法如下表示，因为不惩罚theta_0，所以分为两种情况： 所以其中的梯度表示如下： 12345678910111213def regularized_gradient(theta, X, y, l): """ don't penalize theta_0 args: l: lambda constant return: a vector of gradient """ thetaReg = theta[1:] first = (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y) # 这里人为插入一维0，使得对theta_0不惩罚，方便计算 reg = np.concatenate([np.array([0]), (l / len(X)) * thetaReg]) return first + reg 1.4 One-vs-all Classification这部分我们将实现一对多分类通过训练多个正则化logistic回归分类器，每个对应数据集中K类中的一个。 对于这个任务，我们有10个可能的类，并且由于logistic回归只能一次在2个类之间进行分类，每个分类器在“类别 i”和“不是 i”之间决定。 我们将把分类器训练包含在一个函数中，该函数计算10个分类器中的每个分类器的最终权重，并将权重返回shape为(k, (n+1))数组，其中 n 是参数数量。 12345678910111213141516171819202122from scipy.optimize import minimizedef one_vs_all(X, y, l, K): """generalized logistic regression args: X: feature matrix, (m, n+1) # with incercept x0=1 y: target vector, (m, ) l: lambda constant for regularization K: numbel of labels return: trained parameters """ all_theta = np.zeros((K, X.shape[1])) # (10, 401) for i in range(1, K+1): theta = np.zeros(X.shape[1]) y_i = np.array([1 if label == i else 0 for label in y]) ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method='TNC', jac=regularized_gradient, options=&#123;'disp': True&#125;) all_theta[i-1,:] = ret.x return all_theta 这里需要注意的几点：首先，我们为X添加了一列常数项 1 ，以计算截距项（常数项）。 其次，我们将y从类标签转换为每个分类器的二进制值（要么是类i，要么不是类i）。 最后，我们使用SciPy的较新优化API来最小化每个分类器的代价函数。 如果指定的话，API将采用目标函数，初始参数集，优化方法和jacobian（渐变）函数。 然后将优化程序找到的参数分配给参数数组。 实现向量化代码的一个更具挑战性的部分是正确地写入所有的矩阵，保证维度正确。 12345678910def predict_all(X, all_theta): # compute the class probability for each class on each training instance h = sigmoid(X @ all_theta.T) # 注意的这里的all_theta需要转置 # create array of the index with the maximum probability # Returns the indices of the maximum values along an axis. h_argmax = np.argmax(h, axis=1) # because our array was zero-indexed we need to add one for the true label prediction h_argmax = h_argmax + 1 return h_argmax 这里的h共5000行，10列，每行代表一个样本，每列是预测对应数字的概率。我们取概率最大对应的index加1就是我们分类器最终预测出来的类别。返回的h_argmax是一个array，包含5000个样本对应的预测值。 123456raw_X, raw_y = load_data('ex3data1.mat')X = np.insert(raw_X, 0, 0, axis=1) # (5000, 401)y = raw_y.flatten() # 这里消除了一个维度，方便后面的计算 or .reshape(-1) （5000，）all_theta = one_vs_all(X, y, 1, 10)all_theta # 每一行是一个分类器的一组参数 123y_pred = predict_all(X, all_theta)accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) Tips: python中 true就是1，1就是true，false就是0，0就是false 2 Neural Networks上面使用了多类logistic回归，然而logistic回归不能形成更复杂的假设，因为它只是一个线性分类器。 接下来我们用神经网络来尝试下，神经网络可以实现非常复杂的非线性的模型。我们将利用已经训练好了的权重进行预测。 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123theta1, theta2 = load_weight('ex3weights.mat')theta1.shape, theta2.shape 12345X, y = load_data('ex3data1.mat')y = y.flatten()X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # interceptX.shape, y.shape 123a1 = Xz2 = a1 @ theta1.Tz2.shape 1z2 = np.insert(z2, 0, 1, axis=1) 12a2 = sigmoid(z2)a2.shape 12z3 = a2 @ theta2.Tz3.shape 12a3 = sigmoid(z3)a3.shape 123y_pred = np.argmax(a3, axis=1) + 1 accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) # accuracy = 97.52% 虽然人工神经网络是非常强大的模型，但训练数据的准确性并不能完美预测实际数据，在这里很容易过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(二)：logistic回归]]></title>
    <url>%2F2018%2F05%2F18%2F180523-logistic%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1 Logistic regression在这部分的练习中，你将建立一个逻辑回归模型来预测一个学生是否能进入大学。假设你是一所大学的行政管理人员，你想根据两门考试的结果，来决定每个申请人是否被录取。你有以前申请人的历史数据，可以将其用作逻辑回归训练集。对于每一个训练样本，你有申请人两次测评的分数以及录取的结果。为了完成这个预测任务，我们准备构建一个可以基于两次测试评分来评估录取可能性的分类模型。 1.1 Visualizing the data在开始实现任何学习算法之前，如果可能的话，最好将数据可视化。 1234%matplotlib inlineimport numpy as npimport pandas as pdimport matplotlib.pyplot as plt 12data = pd.read_csv(&apos;ex2data1.txt&apos;, names=[&apos;exam1&apos;, &apos;exam2&apos;, &apos;admitted&apos;])data.head() 1data.describe() 让我们创建两个分数的散点图，并使用颜色编码来可视化，如果样本是正的（被接纳）或负的（未被接纳）。 1234567891011121314positive = data[data.admitted.isin([&apos;1&apos;])] # 1negetive = data[data.admitted.isin([&apos;0&apos;])] # 0fig, ax = plt.subplots(figsize=(6,5))ax.scatter(positive[&apos;exam1&apos;], positive[&apos;exam2&apos;], c=&apos;b&apos;, label=&apos;Admitted&apos;)ax.scatter(negetive[&apos;exam1&apos;], negetive[&apos;exam2&apos;], s=50, c=&apos;r&apos;, marker=&apos;x&apos;, label=&apos;Not Admitted&apos;)# 设置图例显示在图的上方box = ax.get_position()ax.set_position([box.x0, box.y0, box.width , box.height* 0.8])ax.legend(loc=&apos;center left&apos;, bbox_to_anchor=(0.2, 1.12),ncol=3)# 设置横纵坐标名ax.set_xlabel(&apos;Exam 1 Score&apos;)ax.set_ylabel(&apos;Exam 2 Score&apos;)plt.show() 看起来在两类间，有一个清晰的决策边界。现在我们需要实现逻辑回归，那样就可以训练一个模型来预测结果。 1.2 Sigmoid function 首先来回顾下 logistic回归的假设函数： $${{h}_{\theta }}\left( x \right)=g(\theta^{T}x)=\frac{1}{1+{{e}^{-{{\theta }^{T}}X}}}$$ 其中的 g代表一个常用的logistic function为S形函数（Sigmoid function）： $$g\left( z \right)=\frac{1}{1+e^{-z}}$$ 12def sigmoid(z): return 1 / (1 + np.exp(- z)) 让我们做一个快速的检查，来确保它可以工作。 123x1 = np.arange(-10, 10, 0.1)plt.plot(x1, sigmoid(x1), c=&apos;r&apos;)plt.show() 感觉很不错~~~ 1.3 Cost function逻辑回归的代价函数如下，和线性回归的代价函数不一样，因为这个函数是凸的。 $$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}$$ $${{h}_{\theta }}\left( x \right)=g(\theta^{T}x)$$ 1234def cost(theta, X, y): first = (-y) * np.log(sigmoid(X @ theta)) second = (1 - y)*np.log(1 - sigmoid(X @ theta)) return np.mean(first - second) 现在，我们要做一些设置，获取我们的训练集数据。 123456789# add a ones column - this makes the matrix multiplication work out easierif &apos;Ones&apos; not in data.columns: data.insert(0, &apos;Ones&apos;, 1)# set X (training data) and y (target variable)X = data.iloc[:, :-1].as_matrix() # Convert the frame to its Numpy-array representation.y = data.iloc[:, -1].as_matrix() # Return is NOT a Numpy-matrix, rather, a Numpy-array.theta = np.zeros(X.shape[1]) 让我们来检查矩阵的维度，确保一切良好。 12X.shape, theta.shape, y.shape# ((100, 3), (3,), (100,)) 12cost(theta, X, y)# 0.6931471805599453 看起来不错，接下来，我们需要一个函数来计算我们的训练数据、标签和一些参数thate的梯度。 1.4 Gradient * 这是批量梯度下降（batch gradient descent） * 转化为向量化计算： $\frac{1}{m} X^T( Sigmoid(X\theta) - y )$ $$\frac{\partial J\left( \theta \right)}{\partial {{\theta }_{j}}}=\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$$ 123def gradient(theta, X, y): return (X.T @ (sigmoid(X @ theta) - y))/len(X) # the gradient of the cost is a vector of the same length as θ where the jth element (for j = 0, 1, . . . , n) 12gradient(theta, X, y)# array([ -0.1, -12.00921659, -11.26284221]) 1.5 Learning θ parameters注意，我们实际上没有在这个函数中执行梯度下降，我们仅仅在计算梯度。在练习中，一个称为“fminunc”的Octave函数是用来优化函数来计算成本和梯度参数。由于我们使用Python，我们可以用SciPy的“optimize”命名空间来做同样的事情。 这里我们使用的是高级优化算法，运行速度通常远远超过梯度下降。方便快捷。只需传入cost函数，已经所求的变量theta，和梯度。cost函数定义变量时变量tehta要放在第一个，若cost函数只返回cost，则设置fprime=gradient。 1import scipy.optimize as opt 这里使用fimin_tnc或者minimize方法来拟合，minimize中method可以选择不同的算法来计算，其中包括TNC 1234result = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, y))result# (array([-25.16131867, 0.20623159, 0.20147149]), 36, 0) 下面是第二种方法，结果是一样的1234res = opt.minimize(fun=cost, x0=theta, args=(X, y), method=&apos;TNC&apos;, jac=gradient)res# help(opt.minimize) # res.x # final_theta fun: 0.2034977015894744 jac: array([9.11457705e-09, 9.59621025e-08, 4.84073722e-07]) message: &apos;Local minimum reached (|pg| ~= 0)&apos; nfev: 36 nit: 17 status: 0 success: True x: array([-25.16131867, 0.20623159, 0.20147149]) 1cost(result[0], X, y) 0.2034977015894744 1.6 Evaluating logistic regression学习好了参数θ后，我们来用这个模型预测某个学生是否能被录取。 接下来，我们需要编写一个函数，用我们所学的参数theta来为数据集X输出预测。然后，我们可以使用这个函数来给我们的分类器的训练精度打分。 逻辑回归模型的假设函数： ${{h}_{\theta }}\left( x \right)=\frac{1}{1+{{e}^{-{{\theta }^{T}}X}}}$ 当${{h}_{\theta }}$大于等于0.5时，预测 y=1 当${{h}_{\theta }}$小于0.5时，预测 y=0 。 123def predict(theta, X): probability = sigmoid(X@theta) return [1 if x &gt;= 0.5 else 0 for x in probability] # return a list 12345final_theta = result[0]predictions = predict(final_theta, X)correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]accuracy = sum(correct) / len(X)accuracy 0.89 可以看到我们预测精度达到了89%，not bad. 也可以用skearn中的方法来检验。 12from sklearn.metrics import classification_reportprint(classification_report(predictions, y)) precision recall f1-score support 0 0.85 0.87 0.86 39 1 0.92 0.90 0.91 61 avg / total 0.89 0.89 0.89 100 ​ 1.7 Decision boundary（决策边界）$X \times \theta = 0$ (this is the line) $\theta_0 + x_1\theta_1 + x_2\theta_2=0$ 12x1 = np.arange(130, step=0.1)x2 = -(final_theta[0] + x1*final_theta[1]) / final_theta[2] 12345678910fig, ax = plt.subplots(figsize=(8,5))ax.scatter(positive[&apos;exam1&apos;], positive[&apos;exam2&apos;], c=&apos;b&apos;, label=&apos;Admitted&apos;)ax.scatter(negetive[&apos;exam1&apos;], negetive[&apos;exam2&apos;], s=50, c=&apos;r&apos;, marker=&apos;x&apos;, label=&apos;Not Admitted&apos;)ax.plot(x1, x2)ax.set_xlim(0, 130)ax.set_ylim(0, 130)ax.set_xlabel(&apos;x1&apos;)ax.set_ylabel(&apos;x2&apos;)ax.set_title(&apos;Decision Boundary&apos;)plt.show() 2 Regularized logistic regression在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型（在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。这样，我们开始吧。 设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型。 2.1 Visualizing the data12data2 = pd.read_csv(&apos;ex2data2.txt&apos;, names=[&apos;Test 1&apos;, &apos;Test 2&apos;, &apos;Accepted&apos;])data2.head() 123456789101112def plot_data(): positive = data2[data2[&apos;Accepted&apos;].isin([1])] negative = data2[data2[&apos;Accepted&apos;].isin([0])] fig, ax = plt.subplots(figsize=(8,5)) ax.scatter(positive[&apos;Test 1&apos;], positive[&apos;Test 2&apos;], s=50, c=&apos;b&apos;, marker=&apos;o&apos;, label=&apos;Accepted&apos;) ax.scatter(negative[&apos;Test 1&apos;], negative[&apos;Test 2&apos;], s=50, c=&apos;r&apos;, marker=&apos;x&apos;, label=&apos;Rejected&apos;) ax.legend() ax.set_xlabel(&apos;Test 1 Score&apos;) ax.set_ylabel(&apos;Test 2 Score&apos;) plot_data() 注意到其中的正负两类数据并没有线性的决策界限。因此直接用logistic回归在这个数据集上并不能表现良好，因为它只能用来寻找一个线性的决策边界。 所以接下会提到一个新的方法。 2.2 Feature mapping一个拟合数据的更好的方法是从每个数据点创建更多的特征。 我们将把这些特征映射到所有的x1和x2的多项式项上，直到第六次幂。 https://www.zhihu.com/question/65020904123for i in 0..power for p in 0..i: output x1^(i-p) * x2^p $${\rm{mapFeature}}(x)=\begin{bmatrix} {1}\\ {x_1}\\ {x_2}\\ {x_1^2}\\ {x1x2}\\ {x_2^2}\\ {x_1^3}\\ \vdots\\ {x_1x_2^5}\\ {x_2^6} \end{bmatrix}$$ 1234567891011def feature_mapping(x1, x2, power): data = &#123;&#125; for i in np.arange(power + 1): for p in np.arange(i + 1): data["f&#123;&#125;&#123;&#125;".format(i - p, p)] = np.power(x1, i - p) * np.power(x2, p)# data = &#123;"f&#123;&#125;&#123;&#125;".format(i - p, p): np.power(x1, i - p) * np.power(x2, p)# for i in np.arange(power + 1)# for p in np.arange(i + 1)# &#125; return pd.DataFrame(data) 12x1 = data2[&apos;Test 1&apos;].as_matrix()x2 = data2[&apos;Test 2&apos;].as_matrix() 12_data2 = feature_mapping(x1, x2, power=6)_data2.head() 经过映射，我们将有两个特征的向量转化成了一个28维的向量。 在这个高维特征向量上训练的logistic回归分类器将会有一个更复杂的决策边界，当我们在二维图中绘制时，会出现非线性。 虽然特征映射允许我们构建一个更有表现力的分类器，但它也更容易过拟合。在接下来的练习中，我们将实现正则化的logistic回归来拟合数据，并且可以看到正则化如何帮助解决过拟合的问题。 2.3 Regularized Cost function $$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda }{2m}\sum\limits_{j=1}^{n}{\theta _{j}^{2}}$$ 注意：不惩罚第一项$\theta_0$ 先获取特征，标签以及参数theta，确保维度良好。 12345# 这里因为做特征映射的时候已经添加了偏置项，所以不用手动添加了。X = _data2.as_matrix() y = data2['Accepted'].as_matrix()theta = np.zeros(X.shape[1])X.shape, y.shape, theta.shape # ((118, 28), (118,), (28,)) costReg(theta, X, y, l123456def costReg(theta, X, y, l=1): # 不惩罚第一项 _theta = theta[1: ] reg = (l / (2 * len(X))) *(_theta @ _theta) # _theta@_theta == inner product return cost(theta, X, y) + reg 1costReg(theta, X, y, l=1) # 0.6931471805599454 2.4 Regularized gradient 因为我们未对${{\theta }_{0}}$ 进行正则化，所以梯度下降算法将分两种情形： \begin{align} & Repeat\text{ }until\text{ }convergence\text{ }\!\!\{\!\!\text{ } \\ & \text{ }{{\theta }_{0}}:={{\theta }_{0}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{[{{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}}]x_{_{0}}^{(i)}} \\ & \text{ }{{\theta }_{j}}:={{\theta }_{j}}-a\frac{1}{m}\sum\limits_{i=1}^{m}{[{{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}}]x_{j}^{(i)}}+\frac{\lambda }{m}{{\theta }_{j}} \\ & \text{ }\!\!\}\!\!\text{ } \\ & Repeat \\ \end{align} 对上面的算法中 j=1,2,...,n 时的更新式子进行调整可得： ${{\theta }_{j}}:={{\theta }_{j}}(1-a\frac{\lambda }{m})-a\frac{1}{m}\sum\limits_{i=1}^{m}{({{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}})x_{j}^{(i)}}$ 同样不惩罚第一个θ 1234def gradientReg(theta, X, y, l=1): reg = (1 / len(X)) * theta reg[0] = 0 return gradient(theta, X, y) + reg 1gradientReg(theta, X, y, 1) array([8.47457627e-03, 8.47457627e-03, 7.77711864e-05, 3.76648474e-02, 2.34764889e-02, 3.93028171e-02, 3.10079849e-02, 3.87936363e-02, 1.87880932e-02, 1.15013308e-02, 8.19244468e-03, 3.09593720e-03, 4.47629067e-03, 1.37646175e-03, 5.03446395e-02, 7.32393391e-03, 1.28600503e-02, 5.83822078e-03, 7.26504316e-03, 1.83559872e-02, 2.23923907e-03, 3.38643902e-03, 4.08503006e-04, 3.93486234e-02, 4.32983232e-03, 6.31570797e-03, 1.99707467e-02, 1.09740238e-03]) 2.5 Learning parameters12result2 = opt.fmin_tnc(func=costReg, x0=theta, fprime=gradientReg, args=(X, y, 2))result2 (array([ 0.57761135, 0.47056293, 1.09213933, -0.93555548, -0.15107417, -0.96567576, -0.49622178, -0.87226365, 0.5986215 , -0.47857791, -0.19652206, -0.10212812, -0.1513566 , -0.03407832, -1.868297 , -0.25062387, -0.49045048, -0.20293012, -0.26033467, 0.02385201, -0.0290203 , -0.0543879 , 0.01131411, -1.39767636, -0.16559351, -0.24745221, -0.29518657, 0.00854288]), 57, 4) 我们还可以使用高级Python库scikit-learn来解决这个问题。 123from sklearn import linear_model#调用sklearn的线性回归包model = linear_model.LogisticRegression(penalty=&apos;l2&apos;, C=1.0)model.fit(X, y.ravel()) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class=&apos;ovr&apos;, n_jobs=1, penalty=&apos;l2&apos;, random_state=None, solver=&apos;liblinear&apos;, tol=0.0001, verbose=0, warm_start=False) 1model.score(X, y) # 0.8305084745762712 2.6 Evaluating logistic regression12345final_theta = result2[0]predictions = predict(final_theta, X)correct = [1 if a==b else 0 for (a, b) in zip(predictions, y)]accuracy = sum(correct) / len(correct)accuracy 0.8135593220338984 或者用skearn中的方法来评估结果。 12345678910print(classification_report(y, predictions))''' precision recall f1-score support 0 0.87 0.75 0.80 60 1 0.77 0.88 0.82 58 avg / total 0.82 0.81 0.81 118 ''' 可以看到和skearn中的模型精确度差不多，这很不错。 2.6 Decision boundary（决策边界）$X \times \theta = 0$ (this is the line) 12345678910x = np.linspace(-1, 1.5, 250)xx, yy = np.meshgrid(x, x)z = feature_mapping(xx.ravel(), yy.ravel(), 6).as_matrix()z = z @ final_thetaz = z.reshape(xx.shape)plot_data()plt.contour(xx, yy, z, 0)plt.ylim(-.8, 1.2)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(一)：线性回归]]></title>
    <url>%2F2018%2F05%2F18%2F180523-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[##单变量线性回归 在本部分的练习中，您将使用一个变量实现线性回归，以预测食品卡车的利润。假设你是一家餐馆的首席执行官，正在考虑不同的城市开设一个新的分店。该连锁店已经在各个城市拥有卡车，而且你有来自城市的利润和人口数据。您希望使用这些数据来帮助您选择将哪个城市扩展到下一个城市。 12345%matplotlib inlineimport numpy as npimport pandas as pdimport matplotlib.pyplot as plt 导入数据，并查看 1234path = 'ex1data1.txt'# names添加列名，header用指定的行来作为标题，若原无标题且指定标题则设为Nonedata = pd.read_csv(path, header=None, names=['Population', 'Profit']) data.head() 1data.describe() 在开始任何任务之前，通过可视化来理解数据通常是有用的。对于这个数据集，您可以使用散点图来可视化数据，因为它只有两个属性(利润和人口)。(你在现实生活中遇到的许多其他问题都是多维度的，不能在二维图上画出来。) 12data.plot(kind='scatter', x='Population', y='Profit', figsize=(8,5))plt.show() 现在让我们使用梯度下降来实现线性回归，以最小化成本函数。 以下代码示例中实现的方程在“练习”文件夹中的“ex1.pdf”中有详细说明。 首先，我们将创建一个以参数θ为特征函数的代价函数 $$J\left( \theta \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {{h}_{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}}$$ 其中：${{h}_{\theta }}\left( x \right)={{\theta }^{T}}X={{\theta }_{0}}{{x}_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+...+{{\theta }_{n}}{{x}_{n}}$ 计算代价函数 $J(\theta)$ 123def computeCost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X)) 让我们在训练集中添加一列，以便我们可以使用向量化的解决方案来计算代价和梯度。 1data.insert(0, 'Ones', 1) 现在我们来做一些变量初始化。 取最后一列为 y，其余为 X 1234# set X (training data) and y (target variable)cols = data.shape[1] # 列数X = data.iloc[:,0:cols-1] # 取前cols-1列，即输入向量y = data.iloc[:,cols-1:cols] # 取最后一列，即目标向量 观察下 X (训练集) and y (目标变量)是否正确. 1X.head() # head()是观察前5行 1y.head() 注意：这里我使用的是matix而不是array，两者基本通用。 但是matrix的优势就是相对简单的运算符号，比如两个矩阵相乘，就是用符号*，但是array相乘不能这么用，得用方法.dot()array的优势就是不仅仅表示二维，还能表示3、4、5…维，而且在大部分Python程序里，array也是更常用的。 两者区别： 对应元素相乘：matrix可以用np.multiply(X2,X1)，array直接X1*X2 点乘：matrix直接X1*X2，array可以 X1@X2 或 X1.dot(X2) 或 np.dot(X1, X2) 代价函数是应该是numpy矩阵，所以我们需要转换X和Y，然后才能使用它们。 我们还需要初始化theta。 123X = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix([0,0]) theta 是一个(1,2)矩阵 12np.array([[0,0]]).shape # (1, 2) 看下维度，确保计算没问题 12X.shape, theta.shape, y.shape# ((97, 2), (1, 2), (97, 1)) 计算初始代价函数的值 (theta初始值为0). 1computeCost(X, y, theta) # 32.072733877455676 ##batch gradient decent（批量梯度下降） $$ J\left( \theta \right)=\frac{1}{2m}\sum\limits{i=1}^{m}{{{\left( {{h}{\theta }}\left( {{x}^{(i)}} \right)-{{y}^{(i)}} \right)}^{2}}} $$ 其中： $$ {{h}_{\theta }}\left( x \right)={{\theta }^{T}}X={{\theta }_{0}}{{x}_{0}}+{{\theta }_{1}}{{x}_{1}}+{{\theta }_{2}}{{x}_{2}}+...+{{\theta }_{n}}{{x}_{n}} $$ 优化： $$ {{\theta }_{j}}:={{\theta }_{j}}-\alpha \frac{\partial }{\partial {{\theta }_{j}}}J\left( \theta \right) $$ $$\theta_j:=\theta_j-\alpha\frac{1}{m}\sum^{m}{i=1}(h\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$使用 vectorization同时更新所有的 θ，可以大大提高效率 12X.shape, theta.shape, y.shape, X.shape[0]# ((97, 2), (1, 2), (97, 1), 97) 1234567891011121314151617181920212223def gradientDescent(X, y, theta, alpha, epoch): """reuturn theta, cost""" temp = np.matrix(np.zeros(theta.shape)) # 初始化一个 θ 临时矩阵(1, 2) parameters = int(theta.flatten().shape[1]) # 参数 θ的数量 cost = np.zeros(epoch) # 初始化一个ndarray，包含每次epoch的cost m = X.shape[0] # 样本数量m for i in range(epoch): # 利用向量化一步求解 temp =theta - (alpha / m) * (X * theta.T - y).T * X # 以下是不用Vectorization求解梯度下降# error = (X * theta.T) - y # (97, 1) # for j in range(parameters):# term = np.multiply(error, X[:,j]) # (97, 1)# temp[0,j] = theta[0,j] - ((alpha / m) * np.sum(term)) # (1,1) theta = temp cost[i] = computeCost(X, y, theta) return theta, cost 初始化一些附加变量 - 学习速率α和要执行的迭代次数。 12alpha = 0.01epoch = 1000 现在让我们运行梯度下降算法来将我们的参数θ适合于训练集。 1final_theta, cost = gradientDescent(X, y, theta, alpha, epoch) 最后，我们可以使用我们拟合的参数计算训练模型的代价函数（误差）。 1computeCost(X, y, final_theta) 现在我们来绘制线性模型以及数据，直观地看出它的拟合。 np.linspace()在指定的间隔内返回均匀间隔的数字。 1234567891011x = np.linspace(data.Population.min(), data.Population.max(), 100) # 横坐标f = final_theta[0, 0] + (final_theta[0, 1] * x) # 纵坐标，利润fig, ax = plt.subplots(figsize=(6,4))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data['Population'], data.Profit, label='Traning Data')ax.legend(loc=2) # 2表示在左上角ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')plt.show() 由于梯度方程式函数也在每个训练迭代中输出一个代价的向量，所以我们也可以绘制。 请注意，线性回归中的代价函数总是降低的 - 这是凸优化问题的一个例子。 123456fig, ax = plt.subplots(figsize=(8,4))ax.plot(np.arange(epoch), cost, 'r') # np.arange()返回等差数组ax.set_xlabel('Iterations')ax.set_ylabel('Cost')ax.set_title('Error vs. Training Epoch')plt.show() ##多变量线性回归 练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。 123path = 'ex1data2.txt'data2 = pd.read_csv(path, names=['Size', 'Bedrooms', 'Price'])data2.head() 对于此任务，我们添加了另一个预处理步骤 - 特征归一化。 这个对于pandas来说很简单 12data2 = (data2 - data2.mean()) / data2.std()data2.head() 现在我们重复第1部分的预处理步骤，并对新数据集运行线性回归程序。 123456789101112131415161718# add ones columndata2.insert(0, 'Ones', 1)# set X (training data) and y (target variable)cols = data2.shape[1]X2 = data2.iloc[:,0:cols-1]y2 = data2.iloc[:,cols-1:cols]# convert to matrices and initialize thetaX2 = np.matrix(X2.values)y2 = np.matrix(y2.values)theta2 = np.matrix(np.array([0,0,0]))# perform linear regression on the data setg2, cost2 = gradientDescent(X2, y2, theta2, alpha, epoch)# get the cost (error) of the modelcomputeCost(X2, y2, g2), g2 我们也可以快速查看这一个的训练进程。 123456fig, ax = plt.subplots(figsize=(12,8))ax.plot(np.arange(epoch), cost2, 'r')ax.set_xlabel('Iterations')ax.set_ylabel('Cost')ax.set_title('Error vs. Training Epoch')plt.show() 我们也可以使用scikit-learn的线性回归函数，而不是从头开始实现这些算法。 我们将scikit-learn的线性回归算法应用于第1部分的数据，并看看它的表现。 123from sklearn import linear_modelmodel = linear_model.LinearRegression()model.fit(X, y) scikit-learn model的预测表现 1234567891011x = np.array(X[:, 1].A1)f = model.predict(X).flatten()fig, ax = plt.subplots(figsize=(8,5))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Traning Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')plt.show() ##normal equation（正规方程） 正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial }{\partial {{\theta }_{j}}}J\left( {{\theta }_{j}} \right)=0$ 。 假设我们的训练集特征矩阵为 X（包含了${{x}_{0}}=1$）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\theta ={{\left( {{X}^{T}}X \right)}^{-1}}{{X}^{T}}y$ 。 上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={{X}^{T}}X$，则：${{\left( {{X}^{T}}X \right)}^{-1}}={{A}^{-1}}$ 梯度下降与正规方程的比较： 梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型 正规方程：不需要选择学习率α，一次计算得出，需要计算${{\left( {{X}^{T}}X \right)}^{-1}}$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n3)$，通常来说当$n$小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型 1234# 正规方程def normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X) return theta 12final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距final_theta2 1#梯度下降得到的结果是matrix([[-3.24140214, 1.1272942 ]]) 在练习2中，我们将看看分类问题的逻辑回归。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages自定义域名如何支持https]]></title>
    <url>%2F2018%2F05%2F14%2F180514-githubpages%2F</url>
    <content type="text"><![CDATA[就在前不久，GitHub Pages开放了自定义域名支持HTTPS。这意味着广大用GitHub Pages搭建个人博客的同学们有福了，不用再自己买证书或借用第三方服务，就能开启网址左边的小绿锁啦，非常省心。详细信息点我。 最后效果如图： 设置步骤根据你的自定义域名解析类型分为两种：1. CNAME，2. A。 1 CNAME只需要在repositorys设置中开启Enforce HTTPS的选项即可。 若发现复选框为灰色开启不了，可以将Custom domain那一栏的内容删除置为空，点击save保存。再次填入你的自定义域名，点击save保存。会出现这样的提示： 说明你的证书还没发完，耐心等待即可。我等待了三天左右才成功，可能是现在申请的人太多。 2 AA记录的话只需将解析的ip指向如下四个即可，问题官方链接。 1234185.199.108.153185.199.109.153185.199.110.153185.199.111.153 其余的步骤和上面相似。 最重要的是等待，然后你就可以拥有你自己的小绿锁啦。😊]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，熊猫]]></title>
    <url>%2F2018%2F05%2F10%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E7%86%8A%E7%8C%AB%2F</url>
    <content type="text"><![CDATA[哈哈]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
</search>
