<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(五)：偏差和方差]]></title>
    <url>%2F2018%2F05%2F23%2F180523-%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%2F</url>
    <content type="text"><![CDATA[在本练习中，您将实现正则化的线性回归和多项式回归，并使用它来研究具有不同偏差-方差属性的模型 1 Regularized Linear Regression 正则线性回归在前半部分的练习中，你将实现正则化线性回归，以预测水库中的水位变化，从而预测大坝流出的水量。在下半部分中，您将通过一些调试学习算法的诊断，并检查偏差 v.s. 方差的影响。 1.1 Visualizing the dataset我们将从可视化数据集开始，其中包含水位变化的历史记录，x，以及从大坝流出的水量，y。 这个数据集分为了三个部分： training set 训练集：训练模型 cross validation set 交叉验证集：选择正则化参数 test set 测试集：评估性能，模型训练中不曾用过的样本 12345%matplotlib inlineimport numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatimport scipy.optimize as opt 读取数据 123456789101112131415path = 'ex5data1.mat'data = loadmat(path)#Training setX, y = data['X'], data['y']#Cross validation setXval, yval = data['Xval'], data['yval']#Test setXtest, ytest = data['Xtest'], data['ytest']#Insert a column of 1's to all of the X's, as usualX = np.insert(X ,0,1,axis=1)Xval = np.insert(Xval ,0,1,axis=1)Xtest = np.insert(Xtest,0,1,axis=1)print('X=&#123;&#125;,y=&#123;&#125;'.format(X.shape, y.shape))print('Xval=&#123;&#125;,yval=&#123;&#125;'.format(Xval.shape, yval.shape))print('Xtest=&#123;&#125;,ytest=&#123;&#125;'.format(Xtest.shape, ytest.shape)) X=(12, 2),y=(12, 1) Xval=(21, 2),yval=(21, 1) Xtest=(21, 2),ytest=(21, 1) 123456789def plotData(): """瞧一瞧数据长啥样""" plt.figure(figsize=(8,5)) plt.scatter(X[:,1:], y, c='r', marker='x') plt.xlabel('Change in water level (x)') plt.ylabel('Water flowing out of the dam (y)') plt.grid(True) plotData() 1.2 Regularized linear regression cost function 123456789def costReg(theta, X, y, l): '''do not regularizethe theta0 theta is a 1-d array with shape (n+1,) X is a matrix with shape (m, n+1) y is a matrix with shape (m, 1) ''' cost = ((X @ theta - y.flatten()) ** 2).sum() regterm = l * (theta[1:] @ theta[1:]) return (cost + regterm) / (2 * len(X)) Using theta initialized at [1, 1], and lambda = 1, you should expect to see an output of 303.993192 12theta = np.ones(X.shape[1])print(costReg(theta, X, y, 1)) # 303.9931922202643 1.3 Regularized linear regression gradient 12345678910111213141516def gradientReg(theta, X, y, l): """ theta: 1-d array with shape (2,) X: 2-d array with shape (12, 2) y: 2-d array with shape (12, 1) l: lambda constant grad has same shape as theta (2,) """ grad = (X @ theta - y.flatten()) @ X regterm = l * theta regterm[0] = 0 # #don't regulate bias term return (grad + regterm) / len(X)# Using theta initialized at [1; 1] you should expect to see a # gradient of [-15.303016; 598.250744] (with lambda=1)print(gradientReg(theta, X, y, 1)) 1.4 Fitting linear regression 拟合线性回归12345678def trainLinearReg(X, y, l): theta = np.zeros(X.shape[1]) res = opt.minimize(fun=costReg, x0=theta, args=(X, y ,l), method='TNC', jac=gradientReg) return res.x 123fit_theta = trainLinearReg(X, y, 0)plotData()plt.plot(X[:,1], X @ fit_theta) 这里我们把$\lambda$ = 0，因为我们现在实现的线性回归只有两个参数，这么低的维度，正则化并没有用。 从图中可以看到，拟合最好的这条直线告诉我们这个模型并不适合这个数据。 在下一节中，您将实现一个函数来生成学习曲线，它可以帮助您调试学习算法，即使可视化数据不那么容易。 2 Bias-variance机器学习中一个重要的概念是偏差（bias）和方差（variance）的权衡。高偏差意味着欠拟合，高方差意味着过拟合。 在这部分练习中，您将在学习曲线上绘制训练误差和验证误差，以诊断bias-variance问题。 2.1 Learning curves 学习曲线 训练样本X从1开始逐渐增加，训练出不同的参数向量θ。接着通过交叉验证样本Xval计算验证误差。 使用训练集的子集来训练模型，得到不同的theta。 通过theta计算训练代价和交叉验证代价，切记此时不要使用正则化，将 $\lambda = 0$。 计算交叉验证代价时记得整个交叉验证集来计算，无需分为子集。 12345678910111213141516171819def plot_learning_curve(X, y, Xval, yval, l): """画出学习曲线，即交叉验证误差和训练误差随样本数量的变化的变化""" xx = range(1, len(X) + 1) # at least has one example training_cost, cv_cost = [], [] for i in xx: res = trainLinearReg(X[:i], y[:i], l) training_cost_i = costReg(res, X[:i], y[:i], 0) cv_cost_i = costReg(res, Xval, yval, 0) training_cost.append(training_cost_i) cv_cost.append(cv_cost_i) plt.figure(figsize=(8,5)) plt.plot(xx, training_cost, label='training cost') plt.plot(xx, cv_cost, label='cv cost') plt.legend() plt.xlabel('Number of training examples') plt.ylabel('Error') plt.title('Learning curve for linear regression') plt.grid(True) 1learningCurve(X, y, Xval, yval, 0) 从图中看出来，随着样本数量的增加，训练误差和交叉验证误差都很高，这属于高偏差，欠拟合。 3 Polynomial regression 多项式回归我们的线性模型对于数据来说太简单了，导致了欠拟合(高偏差)。在这一部分的练习中，您将通过添加更多的特性来解决这个问题。 使用多项式回归，假设函数形式如下： 3.1 Learning Polynomial Regression数据预处理 X，Xval，Xtest都需要添加多项式特征，这里我们选择增加到6次方，因为若选8次方无法达到作业pdf上的效果图，这是因为scipy和octave版本的优化算法不同。 不要忘了标准化。 12345678910111213141516171819202122def genPolyFeatures(X, power): """添加多项式特征 每次在array的最后一列插入第二列的i+2次方（第一列为偏置） 从二次方开始开始插入（因为本身含有一列一次方） """ Xpoly = X.copy() for i in range(2, power + 1): Xpoly = np.insert(Xpoly, Xpoly.shape[1], np.power(Xpoly[:,1], i), axis=1) return Xpolydef get_means_std(X): """获取训练集的均值和误差，用来标准化所有数据。""" means = np.mean(X,axis=0) stds = np.std(X,axis=0,ddof=1) # ddof=1 means 样本标准差 return means, stdsdef featureNormalize(myX, means, stds): """标准化""" X_norm = myX.copy() X_norm[:,1:] = X_norm[:,1:] - means[1:] X_norm[:,1:] = X_norm[:,1:] / stds[1:] return X_norm 关于归一化，所有数据集应该都用训练集的均值和样本标准差处理。切记。所以要将训练集的均值和样本标准差存储起来，对后面的数据进行处理。 而且注意这里是样本标准差而不是总体标准差，使用np.std()时，将ddof=1则是样本标准差，默认=0是总体标准差。而pandas默认计算样本标准差。 获取添加多项式特征以及 标准化之后的数据。 123456power = 6 # 扩展到x的6次方train_means, train_stds = get_means_std(genPolyFeatures(X,power))X_norm = featureNormalize(genPolyFeatures(X,power), train_means, train_stds)Xval_norm = featureNormalize(genPolyFeatures(Xval,power), train_means, train_stds)Xtest_norm = featureNormalize(genPolyFeatures(Xtest,power), train_means, train_stds) 1234567891011def plot_fit(means, stds, l): """画出拟合曲线""" theta = trainLinearReg(X_norm,y, l) x = np.linspace(-75,55,50) xmat = x.reshape(-1, 1) xmat = np.insert(xmat,0,1,axis=1) Xmat = genPolyFeatures(xmat, power) Xmat_norm = featureNormalize(Xmat, means, stds) plotData() plt.plot(x, Xmat_norm@theta,'b--') 12plot_fit(train_means, train_stds, 0)plot_learning_curve(X_norm, y, Xval_norm, yval, 0) 3.2 Adjusting the regularization parameter上图可以看到 $\lambda$ = 0时，训练误差太小了，明显过拟合了。 我们继续调整$\lambda$ = 1 时： 12plot_fit(train_means, train_stds, 1)plot_learning_curve(X_norm, y, Xval_norm, yval, 1) 我们继续调整$\lambda$ = 100 时，很明显惩罚过多，欠拟合了 12plot_fit(train_means, train_stds, 100)plot_learning_curve(X_norm, y, Xval_norm, yval, 100) 3.3 Selecting λ using a cross validation set1234567891011121314lambdas = [0., 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1., 3., 10.]errors_train, errors_val = [], []for l in lambdas: theta = trainLinearReg(X_norm, y, l) errors_train.append(costReg(theta,X_norm,y,0)) # 记得把lambda = 0 errors_val.append(costReg(theta,Xval_norm,yval,0)) plt.figure(figsize=(8,5))plt.plot(lambdas,errors_train,label='Train')plt.plot(lambdas,errors_val,label='Cross Validation')plt.legend()plt.xlabel('lambda')plt.ylabel('Error')plt.grid(True) 12# 可以看到时交叉验证代价最小的是 lambda = 3lambdas[np.argmin(errors_val)] # 3.0 3.4 Computing test set errorIn our cross validation, we obtained a test error of 3.8599 for λ = 3. 实际上我在上面调整了power=6来匹配作业里面的图，所以得不到3.8599。但是调整power=8时（同作业里一样）,就可以得到上述数据。 12345theta = trainLinearReg(X_norm, y, 3)print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(3, costReg(theta, Xtest_norm, ytest, 0)))# for l in lambdas:# theta = trainLinearReg(X_norm, y, l)# print('test cost(l=&#123;&#125;) = &#123;&#125;'.format(l, costReg(theta, Xtest_norm, ytest, 0))) test cost(l=3) = 4.7552720391599]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(四)：神经网络反向传播]]></title>
    <url>%2F2018%2F05%2F21%2F180521-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1 Neural Networks 神经网络在这个练习中，你将实现反向传播算法来学习神经网络的参数。依旧是上次预测手写数数字的例子。 1.1 Visualizing the data 可视化数据这部分我们随机选取100个样本并可视化。训练集共有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。 12345import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatimport scipy.optimize as optfrom sklearn.metrics import classification_report # 这个包是评价报告 1234567def load_mat(path): '''读取数据''' data = loadmat('ex4data1.mat') # return a dict X = data['X'] y = data['y'].flatten() return X, y 1234567891011def plot_100_images(X): """随机画100个数字""" index = np.random.choice(range(5000), 100) images = X[index] fig, ax_array = plt.subplots(10, 10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow(images[r*10 + c].reshape(20,20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 12X,y = load_mat('ex4data1.mat')plot_100_images(X) 1.2 Model representation 模型表示我们的网络有三层，输入层，隐藏层，输出层。我们的输入是数字图像的像素值，因为每个数字的图像大小为20*20，所以我们输入层有400个单元（这里不包括总是输出要加一个偏置单元）。 1.2.1 load train data set 读取数据首先我们要将标签值（1，2，3，4，…，10）转化成非线性相关的向量，向量对应位置（y[i-1]）上的值等于1，例如y[0]=6转化为y[0]=[0,0,0,0,0,1,0,0,0,0]。 123456789101112131415from sklearn.preprocessing import OneHotEncoderdef expand_y(y): result = [] # 把y中每个类别转化为一个向量，对应的lable值在向量对应位置上置为1 for i in y: y_array = np.zeros(10) y_array[i-1] = 1 result.append(y_array) ''' # 或者用sklearn中OneHotEncoder函数 encoder = OneHotEncoder(sparse=False) # return a array instead of matrix y_onehot = encoder.fit_transform(y.reshape(-1,1)) return y_onehot ''' return np.array(result) 获取训练数据集，以及对训练集做相应的处理，得到我们的input X，lables y。 1234567raw_X, raw_y = load_mat('ex4data1.mat')X = np.insert(raw_X, 0, 1, axis=1)y = expand_y(raw_y)X.shape, y.shape'''((5000, 401), (5000, 10))''' 1.2.2 load weight 读取权重这里我们提供了已经训练好的参数θ1，θ2，存储在ex4weight.mat文件中。这些参数的维度由神经网络的大小决定，第二层有25个单元，输出层有10个单元(对应10个数字类)。 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123t1, t2 = load_weight('ex4weights.mat')t1.shape, t2.shape# ((25, 401), (10, 26)) 1.2.3 展开参数当我们使用高级优化方法来优化神经网络时，我们需要将多个参数矩阵展开，才能传入优化函数，然后再恢复形状。 123def serialize(a, b): '''展开参数''' return np.r_[a.flatten(),b.flatten()] 12theta = serialize(t1, t2) # 扁平化参数，25*401+10*26=10285theta.shape # (10285,) 123def deserialize(seq): '''提取参数''' return seq[:25*401].reshape(25, 401), seq[25*401:].reshape(10, 26) 1.3 Feedforward and cost function 前馈和代价函数1.3.1 Feedforward确保每层的单元数，注意输出时加一个偏置单元，s(1)=400+1，s(2)=25+1，s(3)=10。 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 1234567891011def feed_forward(theta, X,): '''得到每层的输入和输出''' t1, t2 = deserialize(theta) # 前面已经插入过偏置单元，这里就不用插入了 a1 = X z2 = a1 @ t1.T a2 = np.insert(sigmoid(z2), 0, 1, axis=1) z3 = a2 @ t2.T a3 = sigmoid(z3) return a1, z2, a2, z3, a3 1a1, z2, a2, z3, h = feed_forward(theta, X) 1.3.2 Cost function回顾下神经网络的代价函数（不带正则化项） 输出层输出的是对样本的预测，包含5000个数据，每个数据对应了一个包含10个元素的向量，代表了结果有10类。在公式中，每个元素与log项对应相乘。 最后我们使用提供训练好的参数θ，算出的cost应该为0.287629 1234567891011121314def cost(theta, X, y): a1, z2, a2, z3, h = feed_forward(theta, X) J = 0 for i in range(len(X)): first = - y[i] * np.log(h[i]) second = (1 - y[i]) * np.log(1 - h[i]) J = J + np.sum(first - second) J = J / len(X) return J''' # or just use verctorization J = - y * np.log(h) - (1 - y) * np.log(1 - h) return J.sum() / len(X)''' 1cost(theta, X, y) # 0.2876291651613189 1.4 Regularized cost function 正则化代价函数 注意不要将每层的偏置项正则化。 最后You should see that the cost is about 0.383770 12345def regularized_cost(theta, X, y, l=1): '''正则化时忽略每层的偏置项，也就是参数矩阵的第一列''' t1, t2 = deserialize(theta) reg = np.sum(t1[:,1:] ** 2) + np.sum(t2[:,1:] ** 2) # or use np.power(a, 2) return l / (2 * len(X)) * reg + cost(theta, X, y) 1regularized_cost(theta, X, y, 1) # 0.38376985909092354 2 Backpropagation 反向传播2.1 Sigmoid gradient S函数导数 这里可以手动推导，并不难。 12def sigmoid_gradient(z): return sigmoid(z) * (1 - sigmoid(z)) 2.2 Random initialization 随机初始化当我们训练神经网络时，随机初始化参数是很重要的，可以打破数据的对称性。一个有效的策略是在均匀分布(−e，e)中随机选择值，我们可以选择 e = 0.12 这个范围的值来确保参数足够小，使得训练更有效率。 123def random_init(size): '''从服从的均匀分布的范围中随机返回size大小的值''' return np.random.uniform(-0.12, 0.12, size) 2.3 Backpropagation 反向传播 目标：获取整个网络代价函数的梯度。以便在优化算法中求解。 这里面一定要理解正向传播和反向传播的过程，才能弄清楚各种参数在网络中的维度，切记。比如手写出每次传播的式子。 123456789101112print('a1', a1.shape,'t1', t1.shape)print('z2', z2.shape)print('a2', a2.shape, 't2', t2.shape)print('z3', z3.shape)print('a3', h.shape)'''a1 (5000, 401) t1 (25, 401)z2 (5000, 25)a2 (5000, 26) t2 (10, 26)z3 (5000, 10)a3 (5000, 10)''' 1234567891011121314def gradient(theta, X, y): ''' unregularized gradient, notice no d1 since the input layer has no error return 所有参数theta的梯度，故梯度D(i)和参数theta(i)同shape，重要。 ''' t1, t2 = deserialize(theta) a1, z2, a2, z3, h = feed_forward(theta, X) d3 = h - y # (5000, 10) d2 = d3 @ t2[:,1:] * sigmoid_gradient(z2) # (5000, 25) D2 = d3.T @ a2 # (10, 26) D1 = d2.T @ a1 # (25, 401) D = (1 / len(X)) * serialize(D1, D2) # (10285,) return D 2.4 Gradient checking 梯度检测在你的神经网络,你是最小化代价函数J(Θ)。执行梯度检查你的参数,你可以想象展开参数Θ(1)Θ(2)成一个长向量θ。通过这样做,你能使用以下梯度检查过程。 123456789101112131415161718192021def gradient_checking(theta, X, y, e): def a_numeric_grad(plus, minus): """ 对每个参数theta_i计算数值梯度，即理论梯度。 """ return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * 2) numeric_grad = [] for i in range(len(theta)): plus = theta.copy() # deep copy otherwise you will change the raw theta minus = theta.copy() plus[i] = plus[i] + e minus[i] = minus[i] - e grad_i = a_numeric_grad(plus, minus) numeric_grad.append(grad_i) numeric_grad = np.array(numeric_grad) analytic_grad = regularized_gradient(theta, X, y) diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad) print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'.format(diff)) 1gradient_checking(theta, X, y, epsilon= 0.0001)#这个运行很慢，谨慎运行 2.5 Regularized Neural Networks 正则化神经网络 12345678910def regularized_gradient(theta, X, y, l=1): """不惩罚偏置单元的参数""" a1, z2, a2, z3, h = feed_forward(theta, X) D1, D2 = deserialize(gradient(theta, X, y)) t1[:,0] = 0 t2[:,0] = 0 reg_D1 = D1 + (l / len(X)) * t1 reg_D2 = D2 + (l / len(X)) * t2 return serialize(reg_D1, reg_D2) 2.6 Learning parameters using fmincg 优化参数12345678910def nn_training(X, y): init_theta = random_init(10285) # 25*401 + 10*26 res = opt.minimize(fun=regularized_cost, x0=init_theta, args=(X, y, 1), method='TNC', jac=regularized_gradient, options=&#123;'maxiter': 400&#125;) return res 1234567891011121314res = nn_training(X, y)#慢res''' fun: 0.5156784004838036 jac: array([-2.51032294e-04, -2.11248326e-12, 4.38829369e-13, ..., 9.88299811e-05, -2.59923586e-03, -8.52351187e-04]) message: 'Converged (|f_n-f_(n-1)| ~= 0)' nfev: 271 nit: 17 status: 1 success: True x: array([ 0.58440213, -0.02013683, 0.1118854 , ..., -2.8959637 , 1.85893941, -2.78756836])''' 1234def accuracy(theta, X, y): _, _, _, _, h = feed_forward(res.x, X) y_pred = np.argmax(h, axis=1) + 1 print(classification_report(y, y_pred)) 1234567891011121314151617accuracy(res.x, X, raw_y)''' precision recall f1-score support 1 0.97 0.99 0.98 500 2 0.98 0.97 0.98 500 3 0.98 0.95 0.96 500 4 0.98 0.97 0.97 500 5 0.97 0.98 0.97 500 6 0.99 0.98 0.98 500 7 0.99 0.97 0.98 500 8 0.96 0.98 0.97 500 9 0.97 0.98 0.97 500 10 0.99 0.99 0.99 500avg / total 0.98 0.98 0.98 5000''' 3 Visualizing the hidden layer 可视化隐藏层理解神经网络是如何学习的一个很好的办法是，可视化隐藏层单元所捕获的内容。通俗的说，给定一个的隐藏层单元，可视化它所计算的内容的方法是找到一个输入x，x可以激活这个单元（也就是说有一个激活值 $a^{(l)}_i$ 接近与1）。对于我们所训练的网络，注意到θ1中每一行都是一个401维的向量，代表每个隐藏层单元的参数。如果我们忽略偏置项，我们就能得到400维的向量，这个向量代表每个样本输入到每个隐层单元的像素的权重。因此可视化的一个方法是，reshape这个400维的向量为（20，20）的图像然后输出。 注：It turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a norm constraint on the input. 这相当于找到了一个输入，给了隐层单元最高的激活值，给定了一个输入的标准限制。例如( $||x||_2 \leq 1$) (这部分暂时不太理解) 12345678910def plot_hidden(theta): t1, _ = deserialize(theta) t1 = t1[:, 1:] fig,ax_array = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(6,6)) for r in range(5): for c in range(5): ax_array[r, c].matshow(t1[r * 5 + c].reshape(20, 20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1plot_hidden(res.x) 在我们经过训练的网络中，你应该发现隐藏的单元大致对应于在输入中寻找笔画和其他模式的检测器。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BP神经网络(反向传播)详细推导]]></title>
    <url>%2F2018%2F05%2F21%2F180521-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[这篇文章主要讨论神经网络的反向传播的细节，“误差”是如何反向传播的，我们又是如何利用梯度来优化参数的。 在学吴恩达机器学习视频的神经网络那节时，给出了许多公式，比如计算每层的误差，每层参数的梯度，但并没有给出推导过程，可能也是考虑入门级，大多人并不要知道其中含义就可以运用算法了。接下来我会给出详细的推导过程，帮助大家理解。 注意接下来所讲是未正则化的神经网络。 1 计算公式1.1 正向传递假设现在有一个三层的神经网络，如图： 参数含义： $\theta^{(i)}$ 第 $i$ 层的参数矩阵 $z^{(l)}$ 第 $l$ 层的输入 $a^{(l)}$ 第 $l$ 层的输出 传递过程： $a^{(1)}=x​$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ 其中$g$ 为sigmoid激活函数。 1.2 反向传播我们用$\delta^{(l)}$ 表示每层的”误差“，$y$ 为每个样本的标签，$h$ 为每个样本的预测值。 先来从后往前计算每层的“误差“。注意到这里的误差用双引号括起来，因为并不是真正的误差。 $\delta^{(3)}=h-y$ (1) $\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}g^{‘}(z^{(2)})$ (2) 注意第一层是没有误差的，因为是输入层。 吴恩达在课里面提到，”误差“的实质是 $\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}$ ，没错，后面详细说明。 然后来计算每层参数矩阵的梯度，用$\Delta^{(l)}$ 表示 $\Delta^{(2)}=a^{(2)}\delta^{(3)}$ (3) $\Delta^{(1)}=a^{(1)}\delta^{(2)}$ (4) 最后网络的总梯度为： $D=\frac{1}{m}(\Delta^{(1)}+\Delta^{(2)})$ (5) 到这里反向传播就完成了，接着就可以利用梯度下降法或者更高级的优化算法来训练网络。 2 推导这里只推导 $\delta\;和\;\Delta$ 是怎么来的，其余的比较好理解。 首先明确我们要优化的参数有 $\theta^{(1)}$，$\theta^{(2)}$ ，利用梯度下降法的思想，我们只需要求解出代价函数对参数的梯度即可。 假设只有一个输入样本，则代价函数是：$$J(\theta)=-ylogh(x)-(1-y)log(1-h)$$回顾下正向传递的过程，理解其中函数的嵌套关系： $a^{(1)}=x​$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ 然后我们来求解代价函数对参数的梯度，$\frac{\partial}{\partial \theta^{(2)}}J(\theta)$ ，$\frac{\partial}{\partial \theta^{(1)}}J(\theta)$ 。 根据链式求导法则，可以计算得到： 把我画红线的地方令为$\delta^{(3)}$ ，是不是就得到了反向传播中的公式（1）？ 把画绿线的部分令为$\Delta^{(2)}$ ，就得到了公式（3）。我们接着算： 同样把红线部分令为$\delta^{(3)}$ ，紫色部分令为$\delta^{(2)}$ ，就得到了公式（2）。 绿线部分令为$\Delta^{(1)}$ ，就得到了公式（4）。 至此，推导完毕。得到这个规律后，便可以应用到深层次的网络中，计算反向传播时就很方便了。 上面的公式因为书写麻烦，便只写了结果。如果你用笔去慢慢推几分钟，会发现其实很简单。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[吴恩达机器学习作业Python实现(三)：多类分类和前馈神经网络]]></title>
    <url>%2F2018%2F05%2F18%2F180518-logistic%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[在此练习中，我们将使用logistic回归和神经网络来识别手写数字（0到9）。 1 多类分类(多个logistic回归)我们将扩展我们在练习2中写的logistic回归的实现，并将其应用于一对多的分类（不止两个类别）。 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.io import loadmat Dataset首先，加载数据集。这里的数据为MATLAB的格式，所以要使用SciPy.io的loadmat函数。 12345def load_data(path): data = loadmat(path) X = data['X'] y = data['y'] return X,y 12345X, y = load_data('ex3data1.mat')print(np.unique(y)) # 看下有几类标签# [ 1 2 3 4 5 6 7 8 9 10]X.shape, y.shape# ((5000, 400), (5000, 1)) 其中有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。 第一个任务是将我们的逻辑回归实现修改为完全向量化（即没有“for”循环）。这是因为向量化代码除了简洁外，还能够利用线性代数优化，并且通常比迭代代码快得多。 1.2 Visualizing the data123456789101112def plot_an_image(X): """ 随机打印一个数字 """ pick_one = np.random.randint(0, 5000) image = X[pick_one, :] fig, ax = plt.subplots(figsize=(1, 1)) ax.matshow(image.reshape((20, 20)), cmap='gray_r') plt.xticks([]) # 去除刻度，美观 plt.yticks([]) plt.show() print('this should be &#123;&#125;'.format(y[pick_one])) 12345678910111213141516def plot_100_image(X): """ 随机画100个数字 """ sample_idx = np.random.choice(np.arange(X.shape[0]), 100) # 随机选100个样本 sample_images = X[sample_idx, :] # (100,400) fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for row in range(10): for column in range(10): ax_array[row, column].matshow(sample_images[10 * row + column].reshape((20, 20)), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1.3 Vectorizing Logistic Regression我们将使用多个one-vs-all(一对多)logistic回归模型来构建一个多类分类器。由于有10个类，需要训练10个独立的分类器。为了提高训练效率，重要的是向量化。在本节中，我们将实现一个不使用任何for循环的向量化的logistic回归版本。 首先准备下数据。 1.3.1 Vectorizing the cost function首先写出向量化的代价函数。回想正则化的logistic回归的代价函数是： $$ J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j $$ 首先我们对每个样本 $i$ 要计算$h_{\theta}(x^{(i)})$，$h_{\theta}(x^{(i)})=g(\theta^Tx^{(i)})$，$g(z)=\frac{1}{1+e^{-z}}$ sigmoid函数。 事实上我们可以对所有的样本用矩阵乘法来快速的计算。让我们如下来定义 $X$ 和 $\theta$ ： 然后通过计算矩阵积 $X\theta$ ，我们可以得到： 在最后一个等式中，我们用到了一个定理，如果 $a$ 和 $b$ 都是向量，那么 $a^Tb=b^Ta$，这样我们就可以用一行代码计算出所有的样本。 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 123456789101112def regularized_cost(theta, X, y, l): """ don't penalize theta_0 args: X: feature matrix, (m, n+1) # 插入了x0=1 y: target vector, (m, ) l: lambda constant for regularization """ thetaReg = theta[1:] first = (-y*np.log(sigmoid(X@theta))) + (y-1)*np.log(1-sigmoid(X@theta)) reg = (thetaReg@thetaReg)*l / (2*len(X)) return np.mean(first) + reg 1.3.2 Vectorizing the gradient回顾正则化logistic回归代价函数的梯度下降法如下表示，因为不惩罚theta_0，所以分为两种情况： 所以其中的梯度表示如下： 12345678910111213def regularized_gradient(theta, X, y, l): """ don't penalize theta_0 args: l: lambda constant return: a vector of gradient """ thetaReg = theta[1:] first = (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y) # 这里人为插入一维0，使得对theta_0不惩罚，方便计算 reg = np.concatenate([np.array([0]), (l / len(X)) * thetaReg]) return first + reg 1.4 One-vs-all Classification这部分我们将实现一对多分类通过训练多个正则化logistic回归分类器，每个对应数据集中K类中的一个。 对于这个任务，我们有10个可能的类，并且由于logistic回归只能一次在2个类之间进行分类，每个分类器在“类别 i”和“不是 i”之间决定。 我们将把分类器训练包含在一个函数中，该函数计算10个分类器中的每个分类器的最终权重，并将权重返回shape为(k, (n+1))数组，其中 n 是参数数量。 12345678910111213141516171819202122from scipy.optimize import minimizedef one_vs_all(X, y, l, K): """generalized logistic regression args: X: feature matrix, (m, n+1) # with incercept x0=1 y: target vector, (m, ) l: lambda constant for regularization K: numbel of labels return: trained parameters """ all_theta = np.zeros((K, X.shape[1])) # (10, 401) for i in range(1, K+1): theta = np.zeros(X.shape[1]) y_i = np.array([1 if label == i else 0 for label in y]) ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method='TNC', jac=regularized_gradient, options=&#123;'disp': True&#125;) all_theta[i-1,:] = ret.x return all_theta 这里需要注意的几点：首先，我们为X添加了一列常数项 1 ，以计算截距项（常数项）。 其次，我们将y从类标签转换为每个分类器的二进制值（要么是类i，要么不是类i）。 最后，我们使用SciPy的较新优化API来最小化每个分类器的代价函数。 如果指定的话，API将采用目标函数，初始参数集，优化方法和jacobian（渐变）函数。 然后将优化程序找到的参数分配给参数数组。 实现向量化代码的一个更具挑战性的部分是正确地写入所有的矩阵，保证维度正确。 12345678910def predict_all(X, all_theta): # compute the class probability for each class on each training instance h = sigmoid(X @ all_theta.T) # 注意的这里的all_theta需要转置 # create array of the index with the maximum probability # Returns the indices of the maximum values along an axis. h_argmax = np.argmax(h, axis=1) # because our array was zero-indexed we need to add one for the true label prediction h_argmax = h_argmax + 1 return h_argmax 这里的h共5000行，10列，每行代表一个样本，每列是预测对应数字的概率。我们取概率最大对应的index加1就是我们分类器最终预测出来的类别。返回的h_argmax是一个array，包含5000个样本对应的预测值。 123456raw_X, raw_y = load_data('ex3data1.mat')X = np.insert(raw_X, 0, 0, axis=1) # (5000, 401)y = raw_y.flatten() # 这里消除了一个维度，方便后面的计算 or .reshape(-1) （5000，）all_theta = one_vs_all(X, y, 1, 10)all_theta # 每一行是一个分类器的一组参数 123y_pred = predict_all(X, all_theta)accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) Tips: python中 true就是1，1就是true，false就是0，0就是false 2 Neural Networks上面使用了多类logistic回归，然而logistic回归不能形成更复杂的假设，因为它只是一个线性分类器。 接下来我们用神经网络来尝试下，神经网络可以实现非常复杂的非线性的模型。我们将利用已经训练好了的权重进行预测。 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123theta1, theta2 = load_weight('ex3weights.mat')theta1.shape, theta2.shape 12345X, y = load_data('ex3data1.mat')y = y.flatten()X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # interceptX.shape, y.shape 123a1 = Xz2 = a1 @ theta1.Tz2.shape 1z2 = np.insert(z2, 0, 1, axis=1) 12a2 = sigmoid(z2)a2.shape 12z3 = a2 @ theta2.Tz3.shape 12a3 = sigmoid(z3)a3.shape 123y_pred = np.argmax(a3, axis=1) + 1 accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) # accuracy = 97.52% 虽然人工神经网络是非常强大的模型，但训练数据的准确性并不能完美预测实际数据，在这里很容易过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages自定义域名如何支持https]]></title>
    <url>%2F2018%2F05%2F14%2F180514-githubpages%2F</url>
    <content type="text"><![CDATA[就在前不久，GitHub Pages开放了自定义域名支持HTTPS。这意味着广大用GitHub Pages搭建个人博客的同学们有福了，不用再自己买证书或借用第三方服务，就能开启网址左边的小绿锁啦，非常省心。详细信息点我。 最后效果如图： 设置步骤根据你的自定义域名解析类型分为两种：1. CNAME，2. A。 1 CNAME只需要在repositorys设置中开启Enforce HTTPS的选项即可。 若发现复选框为灰色开启不了，可以将Custom domain那一栏的内容删除置为空，点击save保存。再次填入你的自定义域名，点击save保存。会出现这样的提示： 说明你的证书还没发完，耐心等待即可。我等待了三天左右才成功，可能是现在申请的人太多。 2 AA记录的话只需将解析的ip指向如下四个即可，问题官方链接。 1234185.199.108.153185.199.109.153185.199.110.153185.199.111.153 其余的步骤和上面相似。 最重要的是等待，然后你就可以拥有你自己的小绿锁啦。😊]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，熊猫]]></title>
    <url>%2F2018%2F05%2F10%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E7%86%8A%E7%8C%AB%2F</url>
    <content type="text"><![CDATA[哈哈]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
</search>
