<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[å´æ©è¾¾æœºå™¨å­¦ä¹ ä½œä¸šPythonå®žçŽ°(å››)ï¼šç¥žç»ç½‘ç»œåå‘ä¼ æ’­]]></title>
    <url>%2F2018%2F05%2F21%2F180521-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[1 Neural Networks ç¥žç»ç½‘ç»œåœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œä½ å°†å®žçŽ°åå‘ä¼ æ’­ç®—æ³•æ¥å­¦ä¹ ç¥žç»ç½‘ç»œçš„å‚æ•°ã€‚ä¾æ—§æ˜¯ä¸Šæ¬¡é¢„æµ‹æ‰‹å†™æ•°æ•°å­—çš„ä¾‹å­ã€‚ 1.1 Visualizing the data å¯è§†åŒ–æ•°æ®è¿™éƒ¨åˆ†æˆ‘ä»¬éšæœºé€‰å–100ä¸ªæ ·æœ¬å¹¶å¯è§†åŒ–ã€‚è®­ç»ƒé›†å…±æœ‰5000ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯20*20åƒç´ çš„æ•°å­—çš„ç°åº¦å›¾åƒã€‚æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤ºè¯¥ä½ç½®çš„ç°åº¦å¼ºåº¦ã€‚20Ã—20çš„åƒç´ ç½‘æ ¼è¢«å±•å¼€æˆä¸€ä¸ª400ç»´çš„å‘é‡ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µXä¸­ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬éƒ½å˜æˆäº†ä¸€è¡Œï¼Œè¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ª5000Ã—400çŸ©é˜µXï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªæ‰‹å†™æ•°å­—å›¾åƒçš„è®­ç»ƒæ ·æœ¬ã€‚ 12345import numpy as npimport matplotlib.pyplot as pltfrom scipy.io import loadmatimport scipy.optimize as optfrom sklearn.metrics import classification_report # è¿™ä¸ªåŒ…æ˜¯è¯„ä»·æŠ¥å‘Š 1234567def load_mat(path): '''è¯»å–æ•°æ®''' data = loadmat('ex4data1.mat') # return a dict X = data['X'] y = data['y'].flatten() return X, y 1234567891011def plot_100_images(X): """éšæœºç”»100ä¸ªæ•°å­—""" index = np.random.choice(range(5000), 100) images = X[index] fig, ax_array = plt.subplots(10, 10, sharey=True, sharex=True, figsize=(8, 8)) for r in range(10): for c in range(10): ax_array[r, c].matshow(images[r*10 + c].reshape(20,20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 12X,y = load_mat('ex4data1.mat')plot_100_images(X) 1.2 Model representation æ¨¡åž‹è¡¨ç¤ºæˆ‘ä»¬çš„ç½‘ç»œæœ‰ä¸‰å±‚ï¼Œè¾“å…¥å±‚ï¼Œéšè—å±‚ï¼Œè¾“å‡ºå±‚ã€‚æˆ‘ä»¬çš„è¾“å…¥æ˜¯æ•°å­—å›¾åƒçš„åƒç´ å€¼ï¼Œå› ä¸ºæ¯ä¸ªæ•°å­—çš„å›¾åƒå¤§å°ä¸º20*20ï¼Œæ‰€ä»¥æˆ‘ä»¬è¾“å…¥å±‚æœ‰400ä¸ªå•å…ƒï¼ˆè¿™é‡Œä¸åŒ…æ‹¬æ€»æ˜¯è¾“å‡ºè¦åŠ ä¸€ä¸ªåç½®å•å…ƒï¼‰ã€‚ 1.2.1 load train data set è¯»å–æ•°æ®é¦–å…ˆæˆ‘ä»¬è¦å°†æ ‡ç­¾å€¼ï¼ˆ1ï¼Œ2ï¼Œ3ï¼Œ4ï¼Œâ€¦ï¼Œ10ï¼‰è½¬åŒ–æˆéžçº¿æ€§ç›¸å…³çš„å‘é‡ï¼Œå‘é‡å¯¹åº”ä½ç½®ï¼ˆy[i-1]ï¼‰ä¸Šçš„å€¼ç­‰äºŽ1ï¼Œä¾‹å¦‚y[0]=6è½¬åŒ–ä¸ºy[0]=[0,0,0,0,0,1,0,0,0,0]ã€‚ 123456789101112131415from sklearn.preprocessing import OneHotEncoderdef expand_y(y): result = [] # æŠŠyä¸­æ¯ä¸ªç±»åˆ«è½¬åŒ–ä¸ºä¸€ä¸ªå‘é‡ï¼Œå¯¹åº”çš„lableå€¼åœ¨å‘é‡å¯¹åº”ä½ç½®ä¸Šç½®ä¸º1 for i in y: y_array = np.zeros(10) y_array[i-1] = 1 result.append(y_array) ''' # æˆ–è€…ç”¨sklearnä¸­OneHotEncoderå‡½æ•° encoder = OneHotEncoder(sparse=False) # return a array instead of matrix y_onehot = encoder.fit_transform(y.reshape(-1,1)) return y_onehot ''' return np.array(result) èŽ·å–è®­ç»ƒæ•°æ®é›†ï¼Œä»¥åŠå¯¹è®­ç»ƒé›†åšç›¸åº”çš„å¤„ç†ï¼Œå¾—åˆ°æˆ‘ä»¬çš„input Xï¼Œlables yã€‚ 1234567raw_X, raw_y = load_mat('ex4data1.mat')X = np.insert(raw_X, 0, 1, axis=1)y = expand_y(raw_y)X.shape, y.shape'''((5000, 401), (5000, 10))''' 1.2.2 load weight è¯»å–æƒé‡è¿™é‡Œæˆ‘ä»¬æä¾›äº†å·²ç»è®­ç»ƒå¥½çš„å‚æ•°Î¸1ï¼ŒÎ¸2ï¼Œå­˜å‚¨åœ¨ex4weight.matæ–‡ä»¶ä¸­ã€‚è¿™äº›å‚æ•°çš„ç»´åº¦ç”±ç¥žç»ç½‘ç»œçš„å¤§å°å†³å®šï¼Œç¬¬äºŒå±‚æœ‰25ä¸ªå•å…ƒï¼Œè¾“å‡ºå±‚æœ‰10ä¸ªå•å…ƒ(å¯¹åº”10ä¸ªæ•°å­—ç±»)ã€‚ 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123t1, t2 = load_weight('ex4weights.mat')t1.shape, t2.shape# ((25, 401), (10, 26)) 1.2.3 å±•å¼€å‚æ•°å½“æˆ‘ä»¬ä½¿ç”¨é«˜çº§ä¼˜åŒ–æ–¹æ³•æ¥ä¼˜åŒ–ç¥žç»ç½‘ç»œæ—¶ï¼Œæˆ‘ä»¬éœ€è¦å°†å¤šä¸ªå‚æ•°çŸ©é˜µå±•å¼€ï¼Œæ‰èƒ½ä¼ å…¥ä¼˜åŒ–å‡½æ•°ï¼Œç„¶åŽå†æ¢å¤å½¢çŠ¶ã€‚ 123def serialize(a, b): '''å±•å¼€å‚æ•°''' return np.r_[a.flatten(),b.flatten()] 12theta = serialize(t1, t2) # æ‰å¹³åŒ–å‚æ•°ï¼Œ25*401+10*26=10285theta.shape # (10285,) 123def deserialize(seq): '''æå–å‚æ•°''' return seq[:25*401].reshape(25, 401), seq[25*401:].reshape(10, 26) 1.3 Feedforward and cost function å‰é¦ˆå’Œä»£ä»·å‡½æ•°1.3.1 Feedforwardç¡®ä¿æ¯å±‚çš„å•å…ƒæ•°ï¼Œæ³¨æ„è¾“å‡ºæ—¶åŠ ä¸€ä¸ªåç½®å•å…ƒï¼Œs(1)=400+1ï¼Œs(2)=25+1ï¼Œs(3)=10ã€‚ 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 1234567891011def feed_forward(theta, X,): '''å¾—åˆ°æ¯å±‚çš„è¾“å…¥å’Œè¾“å‡º''' t1, t2 = deserialize(theta) # å‰é¢å·²ç»æ’å…¥è¿‡åç½®å•å…ƒï¼Œè¿™é‡Œå°±ä¸ç”¨æ’å…¥äº† a1 = X z2 = a1 @ t1.T a2 = np.insert(sigmoid(z2), 0, 1, axis=1) z3 = a2 @ t2.T a3 = sigmoid(z3) return a1, z2, a2, z3, a3 1a1, z2, a2, z3, h = feed_forward(theta, X) 1.3.2 Cost functionå›žé¡¾ä¸‹ç¥žç»ç½‘ç»œçš„ä»£ä»·å‡½æ•°ï¼ˆä¸å¸¦æ­£åˆ™åŒ–é¡¹ï¼‰ è¾“å‡ºå±‚è¾“å‡ºçš„æ˜¯å¯¹æ ·æœ¬çš„é¢„æµ‹ï¼ŒåŒ…å«5000ä¸ªæ•°æ®ï¼Œæ¯ä¸ªæ•°æ®å¯¹åº”äº†ä¸€ä¸ªåŒ…å«10ä¸ªå…ƒç´ çš„å‘é‡ï¼Œä»£è¡¨äº†ç»“æžœæœ‰10ç±»ã€‚åœ¨å…¬å¼ä¸­ï¼Œæ¯ä¸ªå…ƒç´ ä¸Žlogé¡¹å¯¹åº”ç›¸ä¹˜ã€‚ æœ€åŽæˆ‘ä»¬ä½¿ç”¨æä¾›è®­ç»ƒå¥½çš„å‚æ•°Î¸ï¼Œç®—å‡ºçš„coståº”è¯¥ä¸º0.287629 1234567891011121314def cost(theta, X, y): a1, z2, a2, z3, h = feed_forward(theta, X) J = 0 for i in range(len(X)): first = - y[i] * np.log(h[i]) second = (1 - y[i]) * np.log(1 - h[i]) J = J + np.sum(first - second) J = J / len(X) return J''' # or just use verctorization J = - y * np.log(h) - (1 - y) * np.log(1 - h) return J.sum() / len(X)''' 1cost(theta, X, y) # 0.2876291651613189 1.4 Regularized cost function æ­£åˆ™åŒ–ä»£ä»·å‡½æ•° æ³¨æ„ä¸è¦å°†æ¯å±‚çš„åç½®é¡¹æ­£åˆ™åŒ–ã€‚ æœ€åŽYou should see that the cost is about 0.383770 12345def regularized_cost(theta, X, y, l=1): '''æ­£åˆ™åŒ–æ—¶å¿½ç•¥æ¯å±‚çš„åç½®é¡¹ï¼Œä¹Ÿå°±æ˜¯å‚æ•°çŸ©é˜µçš„ç¬¬ä¸€åˆ—''' t1, t2 = deserialize(theta) reg = np.sum(t1[:,1:] ** 2) + np.sum(t2[:,1:] ** 2) # or use np.power(a, 2) return l / (2 * len(X)) * reg + cost(theta, X, y) 1regularized_cost(theta, X, y, 1) # 0.38376985909092354 2 Backpropagation åå‘ä¼ æ’­2.1 Sigmoid gradient Så‡½æ•°å¯¼æ•° è¿™é‡Œå¯ä»¥æ‰‹åŠ¨æŽ¨å¯¼ï¼Œå¹¶ä¸éš¾ã€‚ 12def sigmoid_gradient(z): return sigmoid(z) * (1 - sigmoid(z)) 2.2 Random initialization éšæœºåˆå§‹åŒ–å½“æˆ‘ä»¬è®­ç»ƒç¥žç»ç½‘ç»œæ—¶ï¼Œéšæœºåˆå§‹åŒ–å‚æ•°æ˜¯å¾ˆé‡è¦çš„ï¼Œå¯ä»¥æ‰“ç ´æ•°æ®çš„å¯¹ç§°æ€§ã€‚ä¸€ä¸ªæœ‰æ•ˆçš„ç­–ç•¥æ˜¯åœ¨å‡åŒ€åˆ†å¸ƒ(âˆ’eï¼Œe)ä¸­éšæœºé€‰æ‹©å€¼ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹© e = 0.12 è¿™ä¸ªèŒƒå›´çš„å€¼æ¥ç¡®ä¿å‚æ•°è¶³å¤Ÿå°ï¼Œä½¿å¾—è®­ç»ƒæ›´æœ‰æ•ˆçŽ‡ã€‚ 123def random_init(size): '''ä»Žæœä»Žçš„å‡åŒ€åˆ†å¸ƒçš„èŒƒå›´ä¸­éšæœºè¿”å›žsizeå¤§å°çš„å€¼''' return np.random.uniform(-0.12, 0.12, size) 2.3 Backpropagation åå‘ä¼ æ’­ ç›®æ ‡ï¼šèŽ·å–æ•´ä¸ªç½‘ç»œä»£ä»·å‡½æ•°çš„æ¢¯åº¦ã€‚ä»¥ä¾¿åœ¨ä¼˜åŒ–ç®—æ³•ä¸­æ±‚è§£ã€‚ è¿™é‡Œé¢ä¸€å®šè¦ç†è§£æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­çš„è¿‡ç¨‹ï¼Œæ‰èƒ½å¼„æ¸…æ¥šå„ç§å‚æ•°åœ¨ç½‘ç»œä¸­çš„ç»´åº¦ï¼Œåˆ‡è®°ã€‚æ¯”å¦‚æ‰‹å†™å‡ºæ¯æ¬¡ä¼ æ’­çš„å¼å­ã€‚ 123456789101112print('a1', a1.shape,'t1', t1.shape)print('z2', z2.shape)print('a2', a2.shape, 't2', t2.shape)print('z3', z3.shape)print('a3', h.shape)'''a1 (5000, 401) t1 (25, 401)z2 (5000, 25)a2 (5000, 26) t2 (10, 26)z3 (5000, 10)a3 (5000, 10)''' 1234567891011121314def gradient(theta, X, y): ''' unregularized gradient, notice no d1 since the input layer has no error return æ‰€æœ‰å‚æ•°thetaçš„æ¢¯åº¦ï¼Œæ•…æ¢¯åº¦D(i)å’Œå‚æ•°theta(i)åŒshapeï¼Œé‡è¦ã€‚ ''' t1, t2 = deserialize(theta) a1, z2, a2, z3, h = feed_forward(theta, X) d3 = h - y # (5000, 10) d2 = d3 @ t2[:,1:] * sigmoid_gradient(z2) # (5000, 25) D2 = d3.T @ a2 # (10, 26) D1 = d2.T @ a1 # (25, 401) D = (1 / len(X)) * serialize(D1, D2) # (10285,) return D 2.4 Gradient checking æ¢¯åº¦æ£€æµ‹åœ¨ä½ çš„ç¥žç»ç½‘ç»œ,ä½ æ˜¯æœ€å°åŒ–ä»£ä»·å‡½æ•°J(Î˜)ã€‚æ‰§è¡Œæ¢¯åº¦æ£€æŸ¥ä½ çš„å‚æ•°,ä½ å¯ä»¥æƒ³è±¡å±•å¼€å‚æ•°Î˜(1)Î˜(2)æˆä¸€ä¸ªé•¿å‘é‡Î¸ã€‚é€šè¿‡è¿™æ ·åš,ä½ èƒ½ä½¿ç”¨ä»¥ä¸‹æ¢¯åº¦æ£€æŸ¥è¿‡ç¨‹ã€‚ 1234567891011121314151617181920def gradient_checking(theta, X, y, e): def a_numeric_grad(plus, minus): """ å¯¹æ¯ä¸ªå‚æ•°theta_iè®¡ç®—æ•°å€¼æ¢¯åº¦ï¼Œå³ç†è®ºæ¢¯åº¦ã€‚ """ return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (e * 2) numeric_grad = [] for i in range(len(theta)): plus = minus = theta plus[i] = plus[i] + e minus[i] = minus[i] - e grad_i = a_numeric_grad(plus, minus) numeric_grad.append(grad_i) numeric_grad = np.array(numeric_grad) analytic_grad = regularized_gradient(theta, X, y) diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad) print('If your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: &#123;&#125;\n'.format(diff)) 1gradient_checking(theta, X, y, epsilon= 0.0001)#è¿™ä¸ªè¿è¡Œå¾ˆæ…¢ï¼Œè°¨æ…Žè¿è¡Œ 2.5 Regularized Neural Networks æ­£åˆ™åŒ–ç¥žç»ç½‘ç»œ 12345678910def regularized_gradient(theta, X, y, l=1): """ä¸æƒ©ç½šåç½®å•å…ƒçš„å‚æ•°""" a1, z2, a2, z3, h = feed_forward(theta, X) D1, D2 = deserialize(gradient(theta, X, y)) t1[:,0] = 0 t2[:,0] = 0 reg_D1 = D1 + (l / len(X)) * t1 reg_D2 = D2 + (l / len(X)) * t2 return serialize(reg_D1, reg_D2) 2.6 Learning parameters using fmincg ä¼˜åŒ–å‚æ•°12345678910def nn_training(X, y): init_theta = random_init(10285) # 25*401 + 10*26 res = opt.minimize(fun=regularized_cost, x0=init_theta, args=(X, y, 1), method='TNC', jac=regularized_gradient, options=&#123;'maxiter': 400&#125;) return res 1234567891011121314res = nn_training(X, y)#æ…¢res''' fun: 0.5156784004838036 jac: array([-2.51032294e-04, -2.11248326e-12, 4.38829369e-13, ..., 9.88299811e-05, -2.59923586e-03, -8.52351187e-04]) message: 'Converged (|f_n-f_(n-1)| ~= 0)' nfev: 271 nit: 17 status: 1 success: True x: array([ 0.58440213, -0.02013683, 0.1118854 , ..., -2.8959637 , 1.85893941, -2.78756836])''' 1234def accuracy(theta, X, y): _, _, _, _, h = feed_forward(res.x, X) y_pred = np.argmax(h, axis=1) + 1 print(classification_report(y, y_pred)) 1234567891011121314151617accuracy(res.x, X, raw_y)''' precision recall f1-score support 1 0.97 0.99 0.98 500 2 0.98 0.97 0.98 500 3 0.98 0.95 0.96 500 4 0.98 0.97 0.97 500 5 0.97 0.98 0.97 500 6 0.99 0.98 0.98 500 7 0.99 0.97 0.98 500 8 0.96 0.98 0.97 500 9 0.97 0.98 0.97 500 10 0.99 0.99 0.99 500avg / total 0.98 0.98 0.98 5000''' 3 Visualizing the hidden layer å¯è§†åŒ–éšè—å±‚ç†è§£ç¥žç»ç½‘ç»œæ˜¯å¦‚ä½•å­¦ä¹ çš„ä¸€ä¸ªå¾ˆå¥½çš„åŠžæ³•æ˜¯ï¼Œå¯è§†åŒ–éšè—å±‚å•å…ƒæ‰€æ•èŽ·çš„å†…å®¹ã€‚é€šä¿—çš„è¯´ï¼Œç»™å®šä¸€ä¸ªçš„éšè—å±‚å•å…ƒï¼Œå¯è§†åŒ–å®ƒæ‰€è®¡ç®—çš„å†…å®¹çš„æ–¹æ³•æ˜¯æ‰¾åˆ°ä¸€ä¸ªè¾“å…¥xï¼Œxå¯ä»¥æ¿€æ´»è¿™ä¸ªå•å…ƒï¼ˆä¹Ÿå°±æ˜¯è¯´æœ‰ä¸€ä¸ªæ¿€æ´»å€¼ $a^{(l)}_i$ æŽ¥è¿‘ä¸Ž1ï¼‰ã€‚å¯¹äºŽæˆ‘ä»¬æ‰€è®­ç»ƒçš„ç½‘ç»œï¼Œæ³¨æ„åˆ°Î¸1ä¸­æ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ª401ç»´çš„å‘é‡ï¼Œä»£è¡¨æ¯ä¸ªéšè—å±‚å•å…ƒçš„å‚æ•°ã€‚å¦‚æžœæˆ‘ä»¬å¿½ç•¥åç½®é¡¹ï¼Œæˆ‘ä»¬å°±èƒ½å¾—åˆ°400ç»´çš„å‘é‡ï¼Œè¿™ä¸ªå‘é‡ä»£è¡¨æ¯ä¸ªæ ·æœ¬è¾“å…¥åˆ°æ¯ä¸ªéšå±‚å•å…ƒçš„åƒç´ çš„æƒé‡ã€‚å› æ­¤å¯è§†åŒ–çš„ä¸€ä¸ªæ–¹æ³•æ˜¯ï¼Œreshapeè¿™ä¸ª400ç»´çš„å‘é‡ä¸ºï¼ˆ20ï¼Œ20ï¼‰çš„å›¾åƒç„¶åŽè¾“å‡ºã€‚ æ³¨ï¼šIt turns out that this is equivalent to finding the input that gives the highest activation for the hidden unit, given a norm constraint on the input. è¿™ç›¸å½“äºŽæ‰¾åˆ°äº†ä¸€ä¸ªè¾“å…¥ï¼Œç»™äº†éšå±‚å•å…ƒæœ€é«˜çš„æ¿€æ´»å€¼ï¼Œç»™å®šäº†ä¸€ä¸ªè¾“å…¥çš„æ ‡å‡†é™åˆ¶ã€‚ä¾‹å¦‚( $||x||_2 \leq 1$) (è¿™éƒ¨åˆ†æš‚æ—¶ä¸å¤ªç†è§£) 12345678910def plot_hidden(theta): t1, _ = deserialize(theta) t1 = t1[:, 1:] fig,ax_array = plt.subplots(5, 5, sharex=True, sharey=True, figsize=(6,6)) for r in range(5): for c in range(5): ax_array[r, c].matshow(t1[r * 5 + c].reshape(20, 20), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1plot_hidden(res.x) åœ¨æˆ‘ä»¬ç»è¿‡è®­ç»ƒçš„ç½‘ç»œä¸­ï¼Œä½ åº”è¯¥å‘çŽ°éšè—çš„å•å…ƒå¤§è‡´å¯¹åº”äºŽåœ¨è¾“å…¥ä¸­å¯»æ‰¾ç¬”ç”»å’Œå…¶ä»–æ¨¡å¼çš„æ£€æµ‹å™¨ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPç¥žç»ç½‘ç»œ(åå‘ä¼ æ’­)è¯¦ç»†æŽ¨å¯¼]]></title>
    <url>%2F2018%2F05%2F21%2F180521-BP%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD)%E8%AF%A6%E7%BB%86%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[è¿™ç¯‡æ–‡ç« ä¸»è¦è®¨è®ºç¥žç»ç½‘ç»œçš„åå‘ä¼ æ’­çš„ç»†èŠ‚ï¼Œâ€œè¯¯å·®â€æ˜¯å¦‚ä½•åå‘ä¼ æ’­çš„ï¼Œæˆ‘ä»¬åˆæ˜¯å¦‚ä½•åˆ©ç”¨æ¢¯åº¦æ¥ä¼˜åŒ–å‚æ•°çš„ã€‚ åœ¨å­¦å´æ©è¾¾æœºå™¨å­¦ä¹ è§†é¢‘çš„ç¥žç»ç½‘ç»œé‚£èŠ‚æ—¶ï¼Œç»™å‡ºäº†è®¸å¤šå…¬å¼ï¼Œæ¯”å¦‚è®¡ç®—æ¯å±‚çš„è¯¯å·®ï¼Œæ¯å±‚å‚æ•°çš„æ¢¯åº¦ï¼Œä½†å¹¶æ²¡æœ‰ç»™å‡ºæŽ¨å¯¼è¿‡ç¨‹ï¼Œå¯èƒ½ä¹Ÿæ˜¯è€ƒè™‘å…¥é—¨çº§ï¼Œå¤§å¤šäººå¹¶ä¸è¦çŸ¥é“å…¶ä¸­å«ä¹‰å°±å¯ä»¥è¿ç”¨ç®—æ³•äº†ã€‚æŽ¥ä¸‹æ¥æˆ‘ä¼šç»™å‡ºè¯¦ç»†çš„æŽ¨å¯¼è¿‡ç¨‹ï¼Œå¸®åŠ©å¤§å®¶ç†è§£ã€‚ æ³¨æ„æŽ¥ä¸‹æ¥æ‰€è®²æ˜¯æœªæ­£åˆ™åŒ–çš„ç¥žç»ç½‘ç»œã€‚ 1 è®¡ç®—å…¬å¼1.1 æ­£å‘ä¼ é€’å‡è®¾çŽ°åœ¨æœ‰ä¸€ä¸ªä¸‰å±‚çš„ç¥žç»ç½‘ç»œï¼Œå¦‚å›¾ï¼š å‚æ•°å«ä¹‰ï¼š $\theta^{(i)}$ ç¬¬ $i$ å±‚çš„å‚æ•°çŸ©é˜µ $z^{(l)}$ ç¬¬ $l$ å±‚çš„è¾“å…¥ $a^{(l)}$ ç¬¬ $l$ å±‚çš„è¾“å‡º ä¼ é€’è¿‡ç¨‹ï¼š $a^{(1)}=xâ€‹$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ å…¶ä¸­$g$ ä¸ºsigmoidæ¿€æ´»å‡½æ•°ã€‚ 1.2 åå‘ä¼ æ’­æˆ‘ä»¬ç”¨$\delta^{(l)}$ è¡¨ç¤ºæ¯å±‚çš„â€è¯¯å·®â€œï¼Œ$y$ ä¸ºæ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼Œ$h$ ä¸ºæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ã€‚ å…ˆæ¥ä»ŽåŽå¾€å‰è®¡ç®—æ¯å±‚çš„â€œè¯¯å·®â€œã€‚æ³¨æ„åˆ°è¿™é‡Œçš„è¯¯å·®ç”¨åŒå¼•å·æ‹¬èµ·æ¥ï¼Œå› ä¸ºå¹¶ä¸æ˜¯çœŸæ­£çš„è¯¯å·®ã€‚ $\delta^{(3)}=h-y$ (1) $\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}g^{â€˜}(z^{(2)})$ (2) æ³¨æ„ç¬¬ä¸€å±‚æ˜¯æ²¡æœ‰è¯¯å·®çš„ï¼Œå› ä¸ºæ˜¯è¾“å…¥å±‚ã€‚ å´æ©è¾¾åœ¨è¯¾é‡Œé¢æåˆ°ï¼Œâ€è¯¯å·®â€œçš„å®žè´¨æ˜¯ $\delta^{(l)}=\frac{\partial J}{\partial z^{(l)}}$ ï¼Œæ²¡é”™ï¼ŒåŽé¢è¯¦ç»†è¯´æ˜Žã€‚ ç„¶åŽæ¥è®¡ç®—æ¯å±‚å‚æ•°çŸ©é˜µçš„æ¢¯åº¦ï¼Œç”¨$\Delta^{(l)}$ è¡¨ç¤º $\Delta^{(2)}=a^{(2)}\delta^{(3)}$ (3) $\Delta^{(1)}=a^{(1)}\delta^{(2)}$ (4) æœ€åŽç½‘ç»œçš„æ€»æ¢¯åº¦ä¸ºï¼š $D=\frac{1}{m}(\Delta^{(1)}+\Delta^{(2)})$ (5) åˆ°è¿™é‡Œåå‘ä¼ æ’­å°±å®Œæˆäº†ï¼ŒæŽ¥ç€å°±å¯ä»¥åˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•æˆ–è€…æ›´é«˜çº§çš„ä¼˜åŒ–ç®—æ³•æ¥è®­ç»ƒç½‘ç»œã€‚ 2 æŽ¨å¯¼è¿™é‡ŒåªæŽ¨å¯¼ $\delta\;å’Œ\;\Delta$ æ˜¯æ€Žä¹ˆæ¥çš„ï¼Œå…¶ä½™çš„æ¯”è¾ƒå¥½ç†è§£ã€‚ é¦–å…ˆæ˜Žç¡®æˆ‘ä»¬è¦ä¼˜åŒ–çš„å‚æ•°æœ‰ $\theta^{(1)}$ï¼Œ$\theta^{(2)}$ ï¼Œåˆ©ç”¨æ¢¯åº¦ä¸‹é™æ³•çš„æ€æƒ³ï¼Œæˆ‘ä»¬åªéœ€è¦æ±‚è§£å‡ºä»£ä»·å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦å³å¯ã€‚ å‡è®¾åªæœ‰ä¸€ä¸ªè¾“å…¥æ ·æœ¬ï¼Œåˆ™ä»£ä»·å‡½æ•°æ˜¯ï¼š$$J(\theta)=-ylogh(x)-(1-y)log(1-h)$$å›žé¡¾ä¸‹æ­£å‘ä¼ é€’çš„è¿‡ç¨‹ï¼Œç†è§£å…¶ä¸­å‡½æ•°çš„åµŒå¥—å…³ç³»ï¼š $a^{(1)}=xâ€‹$ $z^{(2)}=\theta^{(1)}a^{(1)}$ $a^{(2)}=g(z^{(2)}) (add\;a_0^{(2)})$ $z^{(3)}=\theta^{(2)}a^{(2)}$ $h=a^{(3)}=g(z^{(3)})$ ç„¶åŽæˆ‘ä»¬æ¥æ±‚è§£ä»£ä»·å‡½æ•°å¯¹å‚æ•°çš„æ¢¯åº¦ï¼Œ$\frac{\partial}{\partial \theta^{(2)}}J(\theta)$ ï¼Œ$\frac{\partial}{\partial \theta^{(1)}}J(\theta)$ ã€‚ æ ¹æ®é“¾å¼æ±‚å¯¼æ³•åˆ™ï¼Œå¯ä»¥è®¡ç®—å¾—åˆ°ï¼š æŠŠæˆ‘ç”»çº¢çº¿çš„åœ°æ–¹ä»¤ä¸º$\delta^{(3)}$ ï¼Œæ˜¯ä¸æ˜¯å°±å¾—åˆ°äº†åå‘ä¼ æ’­ä¸­çš„å…¬å¼ï¼ˆ1ï¼‰ï¼Ÿ æŠŠç”»ç»¿çº¿çš„éƒ¨åˆ†ä»¤ä¸º$\Delta^{(2)}$ ï¼Œå°±å¾—åˆ°äº†å…¬å¼ï¼ˆ3ï¼‰ã€‚æˆ‘ä»¬æŽ¥ç€ç®—ï¼š åŒæ ·æŠŠçº¢çº¿éƒ¨åˆ†ä»¤ä¸º$\delta^{(3)}$ ï¼Œç´«è‰²éƒ¨åˆ†ä»¤ä¸º$\delta^{(2)}$ ï¼Œå°±å¾—åˆ°äº†å…¬å¼ï¼ˆ2ï¼‰ã€‚ ç»¿çº¿éƒ¨åˆ†ä»¤ä¸º$\Delta^{(1)}$ ï¼Œå°±å¾—åˆ°äº†å…¬å¼ï¼ˆ4ï¼‰ã€‚ è‡³æ­¤ï¼ŒæŽ¨å¯¼å®Œæ¯•ã€‚å¾—åˆ°è¿™ä¸ªè§„å¾‹åŽï¼Œä¾¿å¯ä»¥åº”ç”¨åˆ°æ·±å±‚æ¬¡çš„ç½‘ç»œä¸­ï¼Œè®¡ç®—åå‘ä¼ æ’­æ—¶å°±å¾ˆæ–¹ä¾¿äº†ã€‚ ä¸Šé¢çš„å…¬å¼å› ä¸ºä¹¦å†™éº»çƒ¦ï¼Œä¾¿åªå†™äº†ç»“æžœã€‚å¦‚æžœä½ ç”¨ç¬”åŽ»æ…¢æ…¢æŽ¨å‡ åˆ†é’Ÿï¼Œä¼šå‘çŽ°å…¶å®žå¾ˆç®€å•ã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[å´æ©è¾¾æœºå™¨å­¦ä¹ ä½œä¸šPythonå®žçŽ°(ä¸‰)ï¼šå¤šç±»åˆ†ç±»å’Œå‰é¦ˆç¥žç»ç½‘ç»œ]]></title>
    <url>%2F2018%2F05%2F18%2F180518-logistic%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[åœ¨æ­¤ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨logisticå›žå½’å’Œç¥žç»ç½‘ç»œæ¥è¯†åˆ«æ‰‹å†™æ•°å­—ï¼ˆ0åˆ°9ï¼‰ã€‚ 1 å¤šç±»åˆ†ç±»(å¤šä¸ªlogisticå›žå½’)æˆ‘ä»¬å°†æ‰©å±•æˆ‘ä»¬åœ¨ç»ƒä¹ 2ä¸­å†™çš„logisticå›žå½’çš„å®žçŽ°ï¼Œå¹¶å°†å…¶åº”ç”¨äºŽä¸€å¯¹å¤šçš„åˆ†ç±»ï¼ˆä¸æ­¢ä¸¤ä¸ªç±»åˆ«ï¼‰ã€‚ 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.io import loadmat Dataseté¦–å…ˆï¼ŒåŠ è½½æ•°æ®é›†ã€‚è¿™é‡Œçš„æ•°æ®ä¸ºMATLABçš„æ ¼å¼ï¼Œæ‰€ä»¥è¦ä½¿ç”¨SciPy.ioçš„loadmatå‡½æ•°ã€‚ 12345def load_data(path): data = loadmat(path) X = data['X'] y = data['y'] return X,y 12345X, y = load_data('ex3data1.mat')print(np.unique(y)) # çœ‹ä¸‹æœ‰å‡ ç±»æ ‡ç­¾# [ 1 2 3 4 5 6 7 8 9 10]X.shape, y.shape# ((5000, 400), (5000, 1)) å…¶ä¸­æœ‰5000ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æ˜¯20*20åƒç´ çš„æ•°å­—çš„ç°åº¦å›¾åƒã€‚æ¯ä¸ªåƒç´ ä»£è¡¨ä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œè¡¨ç¤ºè¯¥ä½ç½®çš„ç°åº¦å¼ºåº¦ã€‚20Ã—20çš„åƒç´ ç½‘æ ¼è¢«å±•å¼€æˆä¸€ä¸ª400ç»´çš„å‘é‡ã€‚åœ¨æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µXä¸­ï¼Œæ¯ä¸€ä¸ªæ ·æœ¬éƒ½å˜æˆäº†ä¸€è¡Œï¼Œè¿™ç»™äº†æˆ‘ä»¬ä¸€ä¸ª5000Ã—400çŸ©é˜µXï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯ä¸€ä¸ªæ‰‹å†™æ•°å­—å›¾åƒçš„è®­ç»ƒæ ·æœ¬ã€‚ ç¬¬ä¸€ä¸ªä»»åŠ¡æ˜¯å°†æˆ‘ä»¬çš„é€»è¾‘å›žå½’å®žçŽ°ä¿®æ”¹ä¸ºå®Œå…¨å‘é‡åŒ–ï¼ˆå³æ²¡æœ‰â€œforâ€å¾ªçŽ¯ï¼‰ã€‚è¿™æ˜¯å› ä¸ºå‘é‡åŒ–ä»£ç é™¤äº†ç®€æ´å¤–ï¼Œè¿˜èƒ½å¤Ÿåˆ©ç”¨çº¿æ€§ä»£æ•°ä¼˜åŒ–ï¼Œå¹¶ä¸”é€šå¸¸æ¯”è¿­ä»£ä»£ç å¿«å¾—å¤šã€‚ 1.2 Visualizing the data123456789101112def plot_an_image(X): """ éšæœºæ‰“å°ä¸€ä¸ªæ•°å­— """ pick_one = np.random.randint(0, 5000) image = X[pick_one, :] fig, ax = plt.subplots(figsize=(1, 1)) ax.matshow(image.reshape((20, 20)), cmap='gray_r') plt.xticks([]) # åŽ»é™¤åˆ»åº¦ï¼Œç¾Žè§‚ plt.yticks([]) plt.show() print('this should be &#123;&#125;'.format(y[pick_one])) 12345678910111213141516def plot_100_image(X): """ éšæœºç”»100ä¸ªæ•°å­— """ sample_idx = np.random.choice(np.arange(X.shape[0]), 100) # éšæœºé€‰100ä¸ªæ ·æœ¬ sample_images = X[sample_idx, :] # (100,400) fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for row in range(10): for column in range(10): ax_array[row, column].matshow(sample_images[10 * row + column].reshape((20, 20)), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1.3 Vectorizing Logistic Regressionæˆ‘ä»¬å°†ä½¿ç”¨å¤šä¸ªone-vs-all(ä¸€å¯¹å¤š)logisticå›žå½’æ¨¡åž‹æ¥æž„å»ºä¸€ä¸ªå¤šç±»åˆ†ç±»å™¨ã€‚ç”±äºŽæœ‰10ä¸ªç±»ï¼Œéœ€è¦è®­ç»ƒ10ä¸ªç‹¬ç«‹çš„åˆ†ç±»å™¨ã€‚ä¸ºäº†æé«˜è®­ç»ƒæ•ˆçŽ‡ï¼Œé‡è¦çš„æ˜¯å‘é‡åŒ–ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å®žçŽ°ä¸€ä¸ªä¸ä½¿ç”¨ä»»ä½•forå¾ªçŽ¯çš„å‘é‡åŒ–çš„logisticå›žå½’ç‰ˆæœ¬ã€‚ é¦–å…ˆå‡†å¤‡ä¸‹æ•°æ®ã€‚ 1.3.1 Vectorizing the cost functioné¦–å…ˆå†™å‡ºå‘é‡åŒ–çš„ä»£ä»·å‡½æ•°ã€‚å›žæƒ³æ­£åˆ™åŒ–çš„logisticå›žå½’çš„ä»£ä»·å‡½æ•°æ˜¯ï¼š $$ J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j $$ é¦–å…ˆæˆ‘ä»¬å¯¹æ¯ä¸ªæ ·æœ¬ $i$ è¦è®¡ç®—$h_{\theta}(x^{(i)})$ï¼Œ$h_{\theta}(x^{(i)})=g(\theta^Tx^{(i)})$ï¼Œ$g(z)=\frac{1}{1+e^{-z}}$ sigmoidå‡½æ•°ã€‚ äº‹å®žä¸Šæˆ‘ä»¬å¯ä»¥å¯¹æ‰€æœ‰çš„æ ·æœ¬ç”¨çŸ©é˜µä¹˜æ³•æ¥å¿«é€Ÿçš„è®¡ç®—ã€‚è®©æˆ‘ä»¬å¦‚ä¸‹æ¥å®šä¹‰ $X$ å’Œ $\theta$ ï¼š ç„¶åŽé€šè¿‡è®¡ç®—çŸ©é˜µç§¯ $X\theta$ ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š åœ¨æœ€åŽä¸€ä¸ªç­‰å¼ä¸­ï¼Œæˆ‘ä»¬ç”¨åˆ°äº†ä¸€ä¸ªå®šç†ï¼Œå¦‚æžœ $a$ å’Œ $b$ éƒ½æ˜¯å‘é‡ï¼Œé‚£ä¹ˆ $a^Tb=b^Ta$ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ç”¨ä¸€è¡Œä»£ç è®¡ç®—å‡ºæ‰€æœ‰çš„æ ·æœ¬ã€‚ 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 123456789101112def regularized_cost(theta, X, y, l): """ don't penalize theta_0 args: X: feature matrix, (m, n+1) # æ’å…¥äº†x0=1 y: target vector, (m, ) l: lambda constant for regularization """ thetaReg = theta[1:] first = (-y*np.log(sigmoid(X@theta))) + (y-1)*np.log(1-sigmoid(X@theta)) reg = (thetaReg@thetaReg)*l / (2*len(X)) return np.mean(first) + reg 1.3.2 Vectorizing the gradientå›žé¡¾æ­£åˆ™åŒ–logisticå›žå½’ä»£ä»·å‡½æ•°çš„æ¢¯åº¦ä¸‹é™æ³•å¦‚ä¸‹è¡¨ç¤ºï¼Œå› ä¸ºä¸æƒ©ç½štheta_0ï¼Œæ‰€ä»¥åˆ†ä¸ºä¸¤ç§æƒ…å†µï¼š æ‰€ä»¥å…¶ä¸­çš„æ¢¯åº¦è¡¨ç¤ºå¦‚ä¸‹ï¼š 12345678910111213def regularized_gradient(theta, X, y, l): """ don't penalize theta_0 args: l: lambda constant return: a vector of gradient """ thetaReg = theta[1:] first = (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y) # è¿™é‡Œäººä¸ºæ’å…¥ä¸€ç»´0ï¼Œä½¿å¾—å¯¹theta_0ä¸æƒ©ç½šï¼Œæ–¹ä¾¿è®¡ç®— reg = np.concatenate([np.array([0]), (l / len(X)) * thetaReg]) return first + reg 1.4 One-vs-all Classificationè¿™éƒ¨åˆ†æˆ‘ä»¬å°†å®žçŽ°ä¸€å¯¹å¤šåˆ†ç±»é€šè¿‡è®­ç»ƒå¤šä¸ªæ­£åˆ™åŒ–logisticå›žå½’åˆ†ç±»å™¨ï¼Œæ¯ä¸ªå¯¹åº”æ•°æ®é›†ä¸­Kç±»ä¸­çš„ä¸€ä¸ªã€‚ å¯¹äºŽè¿™ä¸ªä»»åŠ¡ï¼Œæˆ‘ä»¬æœ‰10ä¸ªå¯èƒ½çš„ç±»ï¼Œå¹¶ä¸”ç”±äºŽlogisticå›žå½’åªèƒ½ä¸€æ¬¡åœ¨2ä¸ªç±»ä¹‹é—´è¿›è¡Œåˆ†ç±»ï¼Œæ¯ä¸ªåˆ†ç±»å™¨åœ¨â€œç±»åˆ« iâ€å’Œâ€œä¸æ˜¯ iâ€ä¹‹é—´å†³å®šã€‚ æˆ‘ä»¬å°†æŠŠåˆ†ç±»å™¨è®­ç»ƒåŒ…å«åœ¨ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œè¯¥å‡½æ•°è®¡ç®—10ä¸ªåˆ†ç±»å™¨ä¸­çš„æ¯ä¸ªåˆ†ç±»å™¨çš„æœ€ç»ˆæƒé‡ï¼Œå¹¶å°†æƒé‡è¿”å›žshapeä¸º(k, (n+1))æ•°ç»„ï¼Œå…¶ä¸­ n æ˜¯å‚æ•°æ•°é‡ã€‚ 12345678910111213141516171819202122from scipy.optimize import minimizedef one_vs_all(X, y, l, K): """generalized logistic regression args: X: feature matrix, (m, n+1) # with incercept x0=1 y: target vector, (m, ) l: lambda constant for regularization K: numbel of labels return: trained parameters """ all_theta = np.zeros((K, X.shape[1])) # (10, 401) for i in range(1, K+1): theta = np.zeros(X.shape[1]) y_i = np.array([1 if label == i else 0 for label in y]) ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method='TNC', jac=regularized_gradient, options=&#123;'disp': True&#125;) all_theta[i-1,:] = ret.x return all_theta è¿™é‡Œéœ€è¦æ³¨æ„çš„å‡ ç‚¹ï¼šé¦–å…ˆï¼Œæˆ‘ä»¬ä¸ºXæ·»åŠ äº†ä¸€åˆ—å¸¸æ•°é¡¹ 1 ï¼Œä»¥è®¡ç®—æˆªè·é¡¹ï¼ˆå¸¸æ•°é¡¹ï¼‰ã€‚ å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†yä»Žç±»æ ‡ç­¾è½¬æ¢ä¸ºæ¯ä¸ªåˆ†ç±»å™¨çš„äºŒè¿›åˆ¶å€¼ï¼ˆè¦ä¹ˆæ˜¯ç±»iï¼Œè¦ä¹ˆä¸æ˜¯ç±»iï¼‰ã€‚ æœ€åŽï¼Œæˆ‘ä»¬ä½¿ç”¨SciPyçš„è¾ƒæ–°ä¼˜åŒ–APIæ¥æœ€å°åŒ–æ¯ä¸ªåˆ†ç±»å™¨çš„ä»£ä»·å‡½æ•°ã€‚ å¦‚æžœæŒ‡å®šçš„è¯ï¼ŒAPIå°†é‡‡ç”¨ç›®æ ‡å‡½æ•°ï¼Œåˆå§‹å‚æ•°é›†ï¼Œä¼˜åŒ–æ–¹æ³•å’Œjacobianï¼ˆæ¸å˜ï¼‰å‡½æ•°ã€‚ ç„¶åŽå°†ä¼˜åŒ–ç¨‹åºæ‰¾åˆ°çš„å‚æ•°åˆ†é…ç»™å‚æ•°æ•°ç»„ã€‚ å®žçŽ°å‘é‡åŒ–ä»£ç çš„ä¸€ä¸ªæ›´å…·æŒ‘æˆ˜æ€§çš„éƒ¨åˆ†æ˜¯æ­£ç¡®åœ°å†™å…¥æ‰€æœ‰çš„çŸ©é˜µï¼Œä¿è¯ç»´åº¦æ­£ç¡®ã€‚ 12345678910def predict_all(X, all_theta): # compute the class probability for each class on each training instance h = sigmoid(X @ all_theta.T) # æ³¨æ„çš„è¿™é‡Œçš„all_thetaéœ€è¦è½¬ç½® # create array of the index with the maximum probability # Returns the indices of the maximum values along an axis. h_argmax = np.argmax(h, axis=1) # because our array was zero-indexed we need to add one for the true label prediction h_argmax = h_argmax + 1 return h_argmax è¿™é‡Œçš„hå…±5000è¡Œï¼Œ10åˆ—ï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯åˆ—æ˜¯é¢„æµ‹å¯¹åº”æ•°å­—çš„æ¦‚çŽ‡ã€‚æˆ‘ä»¬å–æ¦‚çŽ‡æœ€å¤§å¯¹åº”çš„indexåŠ 1å°±æ˜¯æˆ‘ä»¬åˆ†ç±»å™¨æœ€ç»ˆé¢„æµ‹å‡ºæ¥çš„ç±»åˆ«ã€‚è¿”å›žçš„h_argmaxæ˜¯ä¸€ä¸ªarrayï¼ŒåŒ…å«5000ä¸ªæ ·æœ¬å¯¹åº”çš„é¢„æµ‹å€¼ã€‚ 123456raw_X, raw_y = load_data('ex3data1.mat')X = np.insert(raw_X, 0, 0, axis=1) # (5000, 401)y = raw_y.flatten() # è¿™é‡Œæ¶ˆé™¤äº†ä¸€ä¸ªç»´åº¦ï¼Œæ–¹ä¾¿åŽé¢çš„è®¡ç®— or .reshape(-1) ï¼ˆ5000ï¼Œï¼‰all_theta = one_vs_all(X, y, 1, 10)all_theta # æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªåˆ†ç±»å™¨çš„ä¸€ç»„å‚æ•° 123y_pred = predict_all(X, all_theta)accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) Tips: pythonä¸­ trueå°±æ˜¯1ï¼Œ1å°±æ˜¯trueï¼Œfalseå°±æ˜¯0ï¼Œ0å°±æ˜¯false 2 Neural Networksä¸Šé¢ä½¿ç”¨äº†å¤šç±»logisticå›žå½’ï¼Œç„¶è€Œlogisticå›žå½’ä¸èƒ½å½¢æˆæ›´å¤æ‚çš„å‡è®¾ï¼Œå› ä¸ºå®ƒåªæ˜¯ä¸€ä¸ªçº¿æ€§åˆ†ç±»å™¨ã€‚ æŽ¥ä¸‹æ¥æˆ‘ä»¬ç”¨ç¥žç»ç½‘ç»œæ¥å°è¯•ä¸‹ï¼Œç¥žç»ç½‘ç»œå¯ä»¥å®žçŽ°éžå¸¸å¤æ‚çš„éžçº¿æ€§çš„æ¨¡åž‹ã€‚æˆ‘ä»¬å°†åˆ©ç”¨å·²ç»è®­ç»ƒå¥½äº†çš„æƒé‡è¿›è¡Œé¢„æµ‹ã€‚ 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123theta1, theta2 = load_weight('ex3weights.mat')theta1.shape, theta2.shape 12345X, y = load_data('ex3data1.mat')y = y.flatten()X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # interceptX.shape, y.shape 123a1 = Xz2 = a1 @ theta1.Tz2.shape 1z2 = np.insert(z2, 0, 1, axis=1) 12a2 = sigmoid(z2)a2.shape 12z3 = a2 @ theta2.Tz3.shape 12a3 = sigmoid(z3)a3.shape 123y_pred = np.argmax(a3, axis=1) + 1 accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) # accuracy = 97.52% è™½ç„¶äººå·¥ç¥žç»ç½‘ç»œæ˜¯éžå¸¸å¼ºå¤§çš„æ¨¡åž‹ï¼Œä½†è®­ç»ƒæ•°æ®çš„å‡†ç¡®æ€§å¹¶ä¸èƒ½å®Œç¾Žé¢„æµ‹å®žé™…æ•°æ®ï¼Œåœ¨è¿™é‡Œå¾ˆå®¹æ˜“è¿‡æ‹Ÿåˆã€‚]]></content>
      <categories>
        <category>æœºå™¨å­¦ä¹ </category>
      </categories>
      <tags>
        <tag>æœºå™¨å­¦ä¹ </tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pagesè‡ªå®šä¹‰åŸŸåå¦‚ä½•æ”¯æŒhttps]]></title>
    <url>%2F2018%2F05%2F14%2F180514-githubpages%2F</url>
    <content type="text"><![CDATA[å°±åœ¨å‰ä¸ä¹…ï¼ŒGitHub Pageså¼€æ”¾äº†è‡ªå®šä¹‰åŸŸåæ”¯æŒHTTPSã€‚è¿™æ„å‘³ç€å¹¿å¤§ç”¨GitHub Pagesæ­å»ºä¸ªäººåšå®¢çš„åŒå­¦ä»¬æœ‰ç¦äº†ï¼Œä¸ç”¨å†è‡ªå·±ä¹°è¯ä¹¦æˆ–å€Ÿç”¨ç¬¬ä¸‰æ–¹æœåŠ¡ï¼Œå°±èƒ½å¼€å¯ç½‘å€å·¦è¾¹çš„å°ç»¿é”å•¦ï¼Œéžå¸¸çœå¿ƒã€‚è¯¦ç»†ä¿¡æ¯ç‚¹æˆ‘ã€‚ æœ€åŽæ•ˆæžœå¦‚å›¾ï¼š è®¾ç½®æ­¥éª¤æ ¹æ®ä½ çš„è‡ªå®šä¹‰åŸŸåè§£æžç±»åž‹åˆ†ä¸ºä¸¤ç§ï¼š1. CNAMEï¼Œ2. Aã€‚ 1 CNAMEåªéœ€è¦åœ¨repositorysè®¾ç½®ä¸­å¼€å¯Enforce HTTPSçš„é€‰é¡¹å³å¯ã€‚ è‹¥å‘çŽ°å¤é€‰æ¡†ä¸ºç°è‰²å¼€å¯ä¸äº†ï¼Œå¯ä»¥å°†Custom domainé‚£ä¸€æ çš„å†…å®¹åˆ é™¤ç½®ä¸ºç©ºï¼Œç‚¹å‡»saveä¿å­˜ã€‚å†æ¬¡å¡«å…¥ä½ çš„è‡ªå®šä¹‰åŸŸåï¼Œç‚¹å‡»saveä¿å­˜ã€‚ä¼šå‡ºçŽ°è¿™æ ·çš„æç¤ºï¼š è¯´æ˜Žä½ çš„è¯ä¹¦è¿˜æ²¡å‘å®Œï¼Œè€å¿ƒç­‰å¾…å³å¯ã€‚æˆ‘ç­‰å¾…äº†ä¸‰å¤©å·¦å³æ‰æˆåŠŸï¼Œå¯èƒ½æ˜¯çŽ°åœ¨ç”³è¯·çš„äººå¤ªå¤šã€‚ 2 AAè®°å½•çš„è¯åªéœ€å°†è§£æžçš„ipæŒ‡å‘å¦‚ä¸‹å››ä¸ªå³å¯ï¼Œé—®é¢˜å®˜æ–¹é“¾æŽ¥ã€‚ 1234185.199.108.153185.199.109.153185.199.110.153185.199.111.153 å…¶ä½™çš„æ­¥éª¤å’Œä¸Šé¢ç›¸ä¼¼ã€‚ æœ€é‡è¦çš„æ˜¯ç­‰å¾…ï¼Œç„¶åŽä½ å°±å¯ä»¥æ‹¥æœ‰ä½ è‡ªå·±çš„å°ç»¿é”å•¦ã€‚ðŸ˜Š]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ä½ å¥½ï¼Œç†ŠçŒ«]]></title>
    <url>%2F2018%2F05%2F10%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E7%86%8A%E7%8C%AB%2F</url>
    <content type="text"><![CDATA[å“ˆå“ˆ]]></content>
      <categories>
        <category>æ—¥å¸¸</category>
      </categories>
      <tags>
        <tag>æ—¥å¸¸</tag>
      </tags>
  </entry>
</search>
