<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Classification]]></title>
    <url>%2F2018%2F05%2F18%2F180518-logistic%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[在此练习中，我们将使用logistic回归和神经网络来识别手写数字（0到9）。 1 多类分类(多个logistic回归)我们将扩展我们在练习2中写的logistic回归的实现，并将其应用于一对多的分类（不止两个类别）。 1234import numpy as npimport pandas as pdimport matplotlib.pyplot as pltfrom scipy.io import loadmat Dataset首先，加载数据集。这里的数据为MATLAB的格式，所以要使用SciPy.io的loadmat函数。 12345def load_data(path): data = loadmat(path) X = data['X'] y = data['y'] return X,y 12345X, y = load_data('ex3data1.mat')print(np.unique(y)) # 看下有几类标签# [ 1 2 3 4 5 6 7 8 9 10]X.shape, y.shape# ((5000, 400), (5000, 1)) 其中有5000个训练样本，每个样本是20*20像素的数字的灰度图像。每个像素代表一个浮点数，表示该位置的灰度强度。20×20的像素网格被展开成一个400维的向量。在我们的数据矩阵X中，每一个样本都变成了一行，这给了我们一个5000×400矩阵X，每一行都是一个手写数字图像的训练样本。 第一个任务是将我们的逻辑回归实现修改为完全向量化（即没有“for”循环）。这是因为向量化代码除了简洁外，还能够利用线性代数优化，并且通常比迭代代码快得多。 1.2 Visualizing the data123456789101112def plot_an_image(X): """ 随机打印一个数字 """ pick_one = np.random.randint(0, 5000) image = X[pick_one, :] fig, ax = plt.subplots(figsize=(1, 1)) ax.matshow(image.reshape((20, 20)), cmap='gray_r') plt.xticks([]) # 去除刻度，美观 plt.yticks([]) plt.show() print('this should be &#123;&#125;'.format(y[pick_one])) 12345678910111213141516def plot_100_image(X): """ 随机画100个数字 """ sample_idx = np.random.choice(np.arange(X.shape[0]), 100) # 随机选100个样本 sample_images = X[sample_idx, :] # (100,400) fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8)) for row in range(10): for column in range(10): ax_array[row, column].matshow(sample_images[10 * row + column].reshape((20, 20)), cmap='gray_r') plt.xticks([]) plt.yticks([]) plt.show() 1.3 Vectorizing Logistic Regression我们将使用多个one-vs-all(一对多)logistic回归模型来构建一个多类分类器。由于有10个类，需要训练10个独立的分类器。为了提高训练效率，重要的是向量化。在本节中，我们将实现一个不使用任何for循环的向量化的logistic回归版本。 首先准备下数据。 1.3.1 Vectorizing the cost function首先写出向量化的代价函数。回想正则化的logistic回归的代价函数是： $$ J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( {{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1-{{h}_{\theta }}\left( {{x}^{(i)}} \right) \right)]}+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j $$ 首先我们对每个样本 $i$ 要计算$h_{\theta}(x^{(i)})$，$h_{\theta}(x^{(i)})=g(\theta^Tx^{(i)})$，$g(z)=\frac{1}{1+e^{-z}}$ sigmoid函数。 事实上我们可以对所有的样本用矩阵乘法来快速的计算。让我们如下来定义 $X$ 和 $\theta$ ： 然后通过计算矩阵积 $X\theta$ ，我们可以得到： 在最后一个等式中，我们用到了一个定理，如果 $a$ 和 $b$ 都是向量，那么 $a^Tb=b^Ta$，这样我们就可以用一行代码计算出所有的样本。 12def sigmoid(z): return 1 / (1 + np.exp(-z)) 123456789101112def regularized_cost(theta, X, y, l): """ don't penalize theta_0 args: X: feature matrix, (m, n+1) # 插入了x0=1 y: target vector, (m, ) l: lambda constant for regularization """ thetaReg = theta[1:] first = (-y*np.log(sigmoid(X@theta))) + (y-1)*np.log(1-sigmoid(X@theta)) reg = (thetaReg@thetaReg)*l / (2*len(X)) return np.mean(first) + reg 1.3.2 Vectorizing the gradient回顾正则化logistic回归代价函数的梯度下降法如下表示，因为不惩罚theta_0，所以分为两种情况： 所以其中的梯度表示如下： 12345678910111213def regularized_gradient(theta, X, y, l): """ don't penalize theta_0 args: l: lambda constant return: a vector of gradient """ thetaReg = theta[1:] first = (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y) # 这里人为插入一维0，使得对theta_0不惩罚，方便计算 reg = np.concatenate([np.array([0]), (l / len(X)) * thetaReg]) return first + reg 1.4 One-vs-all Classification这部分我们将实现一对多分类通过训练多个正则化logistic回归分类器，每个对应数据集中K类中的一个。 对于这个任务，我们有10个可能的类，并且由于logistic回归只能一次在2个类之间进行分类，每个分类器在“类别 i”和“不是 i”之间决定。 我们将把分类器训练包含在一个函数中，该函数计算10个分类器中的每个分类器的最终权重，并将权重返回shape为(k, (n+1))数组，其中 n 是参数数量。 12345678910111213141516171819202122from scipy.optimize import minimizedef one_vs_all(X, y, l, K): """generalized logistic regression args: X: feature matrix, (m, n+1) # with incercept x0=1 y: target vector, (m, ) l: lambda constant for regularization K: numbel of labels return: trained parameters """ all_theta = np.zeros((K, X.shape[1])) # (10, 401) for i in range(1, K+1): theta = np.zeros(X.shape[1]) y_i = np.array([1 if label == i else 0 for label in y]) ret = minimize(fun=regularized_cost, x0=theta, args=(X, y_i, l), method='TNC', jac=regularized_gradient, options=&#123;'disp': True&#125;) all_theta[i-1,:] = ret.x return all_theta 这里需要注意的几点：首先，我们为X添加了一列常数项 1 ，以计算截距项（常数项）。 其次，我们将y从类标签转换为每个分类器的二进制值（要么是类i，要么不是类i）。 最后，我们使用SciPy的较新优化API来最小化每个分类器的代价函数。 如果指定的话，API将采用目标函数，初始参数集，优化方法和jacobian（渐变）函数。 然后将优化程序找到的参数分配给参数数组。 实现向量化代码的一个更具挑战性的部分是正确地写入所有的矩阵，保证维度正确。 12345678910def predict_all(X, all_theta): # compute the class probability for each class on each training instance h = sigmoid(X @ all_theta.T) # 注意的这里的all_theta需要转置 # create array of the index with the maximum probability # Returns the indices of the maximum values along an axis. h_argmax = np.argmax(h, axis=1) # because our array was zero-indexed we need to add one for the true label prediction h_argmax = h_argmax + 1 return h_argmax 这里的h共5000行，10列，每行代表一个样本，每列是预测对应数字的概率。我们取概率最大对应的index加1就是我们分类器最终预测出来的类别。返回的h_argmax是一个array，包含5000个样本对应的预测值。 123456raw_X, raw_y = load_data('ex3data1.mat')X = np.insert(raw_X, 0, 0, axis=1) # (5000, 401)y = raw_y.flatten() # 这里消除了一个维度，方便后面的计算 or .reshape(-1) （5000，）all_theta = one_vs_all(X, y, 1, 10)all_theta # 每一行是一个分类器的一组参数 123y_pred = predict_all(X, all_theta)accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) Tips: python中 true就是1，1就是true，false就是0，0就是false 2 Neural Networks上面使用了多类logistic回归，然而logistic回归不能形成更复杂的假设，因为它只是一个线性分类器。 接下来我们用神经网络来尝试下，神经网络可以实现非常复杂的非线性的模型。我们将利用已经训练好了的权重进行预测。 123def load_weight(path): data = loadmat(path) return data['Theta1'], data['Theta2'] 123theta1, theta2 = load_weight('ex3weights.mat')theta1.shape, theta2.shape 12345X, y = load_data('ex3data1.mat')y = y.flatten()X = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1) # interceptX.shape, y.shape 123a1 = Xz2 = a1 @ theta1.Tz2.shape 1z2 = np.insert(z2, 0, 1, axis=1) 12a2 = sigmoid(z2)a2.shape 12z3 = a2 @ theta2.Tz3.shape 12a3 = sigmoid(z3)a3.shape 123y_pred = np.argmax(a3, axis=1) + 1 accuracy = np.mean(y_pred == y)print ('accuracy = &#123;0&#125;%'.format(accuracy * 100)) # accuracy = 97.52% 虽然人工神经网络是非常强大的模型，但训练数据的准确性并不能完美预测实际数据，在这里很容易过拟合。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub Pages自定义域名如何支持https]]></title>
    <url>%2F2018%2F05%2F14%2F180514-githubpages%2F</url>
    <content type="text"><![CDATA[就在前不久，GitHub Pages开放了自定义域名支持HTTPS。这意味着广大用GitHub Pages搭建个人博客的同学们有福了，不用再自己买证书或借用第三方服务，就能开启网址左边的小绿锁啦，非常省心。详细信息点我。 最后效果如图： 设置步骤根据你的自定义域名解析类型分为两种：1. CNAME，2. A。 1 CNAME只需要在repositorys设置中开启Enforce HTTPS的选项即可。 若发现复选框为灰色开启不了，可以将Custom domain那一栏的内容删除置为空，点击save保存。再次填入你的自定义域名，点击save保存。会出现这样的提示： 说明你的证书还没发完，耐心等待即可。我等待了三天左右才成功，可能是现在申请的人太多。 2 AA记录的话只需将解析的ip指向如下四个即可，问题官方链接。 1234185.199.108.153185.199.109.153185.199.110.153185.199.111.153 其余的步骤和上面相似。 最重要的是等待，然后你就可以拥有你自己的小绿锁啦。😊]]></content>
      <categories>
        <category>Github</category>
      </categories>
      <tags>
        <tag>Github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你好，熊猫]]></title>
    <url>%2F2018%2F05%2F10%2F%E4%BD%A0%E5%A5%BD%EF%BC%8C%E7%86%8A%E7%8C%AB%2F</url>
    <content type="text"><![CDATA[哈哈]]></content>
      <categories>
        <category>日常</category>
      </categories>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
</search>
